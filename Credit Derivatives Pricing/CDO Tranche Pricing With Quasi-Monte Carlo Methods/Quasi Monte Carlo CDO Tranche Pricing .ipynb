{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332898df",
   "metadata": {},
   "source": [
    "# QUASI MONTE CARLO CDO PRICING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2089dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import t\n",
    "from scipy.stats import norm, norminvgauss\n",
    "from scipy.stats.qmc import Sobol\n",
    "from scipy.integrate import quad\n",
    "from primePy import primes\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"numpy.core.fromnumeric\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"numpy.core._methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfdcb3",
   "metadata": {},
   "source": [
    "### Les méthodes de Quasi Monte Carlo : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b848bfd",
   "metadata": {},
   "source": [
    "Les méthodes de quasi monte-carlo sont des méthodes qui supposent de remplacer les nombres pseudo-aléatoires par des séquences déterministes. Autrement, les nombres pseudo-aléatoires sont remplacés par des séquences déterministes de nombres qui une fois remplacés stricto-sensus en lieu et place des nombres pseudo-alétoires, permettent de réduire les temps de simulation, d'avoir une meilleure approximation et surtout d'avoir un control sur nos simulations. \n",
    "\n",
    "\n",
    "Les séquences déterministes de nombres qui remplacent les nombres pseudo-aléatoires sont appelés des **suites a discrépance faible**, ie des séquences dont la propriété est de bien occuper l'espace \\[0,1\\]. Parmi ces suites, nous avons : \n",
    "\n",
    "\n",
    "**Suite de Van der Corput**\n",
    "La suite de Van der Corput repose sur la manipulation des coefficients de la décomposition $p-$adique de $n \\ge 1$:\n",
    "- Soit $p$ un nombre premier qui sert de base à la décomposition $p-$adique\n",
    "- Décomposition de $n$: \n",
    "\\begin{equation*}\n",
    "    n = a_0 + a_1 p + \\dots a_r p^r, \\quad a_k \\in \\{0, \\dots, p-1\\}, \\quad a_r \\neq 0,\n",
    "\\end{equation*}\n",
    "- Construction de $\\xi_n$: \n",
    "\\begin{equation*}\n",
    "    \\xi^{(p)}_n = \\frac{a_0}{p} + \\frac{a_1}{p^2} + \\dots + \\frac{a_r}{p^{r+1}} \\in [0,1].\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "    D_n^*(\\xi^{(p)}) \\le \\frac{1}{n} \\left(1 + (p-1) \\frac{\\log(p n)}{\\log(p)}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "**Suite de Halton**\n",
    "\n",
    "C'est une Van der Corput multidimensionnelle.\n",
    "Soit $d$ la dimension, et $p_1,\\dots, p_d$ les $d$ premiers nombres premiers.\n",
    "\n",
    "La suite d'Halton est définie par  \n",
    "\\begin{equation*}\n",
    "    \\Xi^{(d)}_n = (\\xi^{(p_1)}_n, \\dots, \\xi^{(p_d)}_n)\n",
    "\\end{equation*}\n",
    "\n",
    "**Discrépance:** \n",
    "\\begin{equation*}\n",
    "    D_n^*(\\xi^{(p)}) \\le \\frac{1}{n} \\left(1 + \\prod_{i=1}^d(p_i-1) \\frac{\\log(p_i n)}{\\log(p_i)}\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "**Suite de Sobol**\n",
    "\n",
    "L'algorithme le plus populaire pour les suites à discrépances faible. La génération d'une suite de Sobol est très rapide car toute l'arithmétique se fait en base `2` (à la différence de la suite de Halton).\n",
    "Il est recommandé d'utiliser uniquement des algorithmes déjà codés, testés et éprouvés! Il ne faut pas recoder son prore algorithme qui sera peut-être invalide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b14d9",
   "metadata": {},
   "source": [
    "**Halton & Copule Gaussienne a 1 facteur et Approximation LHP**\n",
    "\n",
    "\n",
    "### Classe Quasi\\_Monte\\_Carlo\\_Gaussian\\_CDO:\n",
    "\n",
    "\n",
    "$\\_\\_init\\_\\_$ initialise les paramètres de la classe avec les arguments donnés:\n",
    "\n",
    " - attachment : Point d'attachement\n",
    " - detachment : Point de détachement\n",
    " - rho : Coefficient de corrélation\n",
    " - intensity : Intensité de défaut\n",
    " - T : Horizon de temps\n",
    " - d : Dimension de l'espace des variables aléatoires\n",
    " - r : Taux d'intérêt sans risque\n",
    " - notionel : Montant notionnel du CDO\n",
    " - n\\_samples : Nombre d'échantillons pour la simulation\n",
    "\n",
    "  \n",
    "- Génération des échantillons Quasi-Monte Carlo:\n",
    "\n",
    "  La méthode **quasi_monte_carlo_stopping_times** génère des temps d'arrêt corrélés en utilisant la méthode Quasi-Monte Carlo. Les étapes sont les suivantes :\n",
    "\n",
    "    - Génération de la suite de van der Corput avec la fonction van_der_corput_r.\n",
    "    - Génération de la suite de van der Corput pour n échantillons et une base p avec la fonction van_der_corput.\n",
    "    -  Génération de la suite de Halton pour n échantillons et d dimensions avec la fonction halton.\n",
    "    -  Transformation Box-Muller pour générer des échantillons multivariés à partir des échantillons quasi-aléatoires avec la fonction box_muller_multivariate_sample.\n",
    "    -  Génération des échantillons multivariés corrélés en utilisant la décomposition de Cholesky avec la fonction correlated_brownian_multivariate_sample.\n",
    "    -  Filtrage des temps d'arrêt pour ne garder que ceux inférieurs à l'horizon de temps T.\n",
    "\n",
    "\n",
    "- Probabilités neutres au risque:\n",
    "\n",
    "  La méthode **risk_neutral_probabilities** calcule la probabilité neutre au risque à un instant t donné.\n",
    "\n",
    "\n",
    "-  Perte attendue:\n",
    "\n",
    "   La méthode expected_loss calcule la perte attendue à un instant t en utilisant une distribution bivariée normale. Nous avons utilisé la meme expected loss que celle dans le cas de la copule gaussienne avec une copule gaussienne.\n",
    "\n",
    "\n",
    "\n",
    "-  Calcul du spread du CDO synthétique:\n",
    "\n",
    "     La méthode **synthetic_cdo_spread_computation_gaussian_normal** calcule le spread du CDO synthétique en utilisant l'approche Quasi-Monte Carlo. Les étapes sont les suivantes :\n",
    "     \n",
    "   - Génération des temps d'arrêt avec la méthode quasi_monte_carlo_stopping_times.\n",
    "\n",
    "   -  Pour chaque échantillon de temps d'arrêt, calcul des flux de prime et des flux de protection en utilisant la perte attendue et les probabilités neutres au risque et en considérant le taux de recouvrement.\n",
    "\n",
    "   -  Calcul du spread pour chaque échantillon en divisant la somme des flux de prime par la somme des flux de protection. Ajout d'un petit nombre epsilon pour éviter les divisions par zéro.\n",
    "\n",
    "   -  Calcul de la moyenne, de la variance et de l'intervalle de confiance des spreads obtenus à partir des échantillons.\n",
    "\n",
    "   -  Retour des résultats sous forme d'un DataFrame pandas.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9274e202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quasi_Monte_Carlo_Gaussian_CDO:\n",
    "    \n",
    "    def __init__(self, attachment, detachment, rho, intensity, T, d, r, notionel, n_samples):\n",
    "        \n",
    "        self.attachment = attachment\n",
    "        self.detachment = detachment\n",
    "        self.rho = rho\n",
    "        self.intensity = intensity\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.r = r\n",
    "        self.notionel = notionel\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "    def quasi_monte_carlo_stopping_times(self):\n",
    "        \n",
    "        def van_der_corput_r(r: int = 1, p: int = 2, scramble: bool = True):\n",
    "            rng = np.random.default_rng() \n",
    "            coeffs = np.arange(p)[np.newaxis]\n",
    "            if scramble:\n",
    "                rng.shuffle(coeffs, axis=1)\n",
    "            seq = coeffs / p\n",
    "            for k in range(2, r+1):\n",
    "                if scramble:\n",
    "                    rng.shuffle(coeffs, axis=1)\n",
    "                seq = (seq + coeffs.T/p**k).reshape((1, -1))\n",
    "            seq = seq.flatten()\n",
    "            return seq\n",
    "\n",
    "        def van_der_corput(n=self.n_samples, p=self.d):\n",
    "            r = np.ceil(np.log(n) / np.log(p))\n",
    "            seq = van_der_corput_r(int(r), p, scramble=True)\n",
    "            return seq[:n]\n",
    "\n",
    "        def halton(n=self.n_samples, d=self.d):\n",
    "            n_primes = primes.first(self.d + 4) \n",
    "            result = [van_der_corput(n, n_primes[k]) for k in range(d)]\n",
    "            return np.array(result).T\n",
    "\n",
    "        def box_muller_multivariate_sample():\n",
    "            quasi_random_samples = halton()\n",
    "            epsilon = 1e-10  \n",
    "            array = np.empty((self.n_samples,self.d))\n",
    "\n",
    "            for i in range(0,int(quasi_random_samples.shape[1]/2)):\n",
    "              \n",
    "                array[:, 2 * i] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.cos(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "                array[:, 2 * i + 1] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.sin(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "            return array\n",
    "\n",
    "        \n",
    "        def correlated_brownian_multivariate_sample():\n",
    "            \n",
    "            multivariate_samples = box_muller_multivariate_sample()\n",
    "            \n",
    "            covariance_matrix = np.full((self.d,self.d),self.rho) + (1 - self.rho) * np.eye(self.d)\n",
    "            \n",
    "            if np.allclose(covariance_matrix,covariance_matrix.T):\n",
    "                try : \n",
    "                    L = np.linalg.cholesky(covariance_matrix)\n",
    "\n",
    "\n",
    "                except np.linalg.LinAlgError : \n",
    "                    print(\"La matrice n'est pas sémi définie positive\")\n",
    "                \n",
    "            # Une fois obtenue L , nous pouvons trouver l'échantillon final correspondant \n",
    "            multivariate_samples_correlated = L @ multivariate_samples.T\n",
    "        \n",
    "            multivariate_samples_correlated = multivariate_samples_correlated.T\n",
    "            \n",
    "            \n",
    "            # simulons les temps d'arret\n",
    "        \n",
    "            stopping_times = (-1/self.intensity) * np.log(norm.cdf(multivariate_samples_correlated))\n",
    "        \n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "            \n",
    "                stopping_times[i] = np.sort(stopping_times[i])\n",
    "            \n",
    "            filtered_stopping_times = []\n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "                filtered_times = stopping_times[i][stopping_times[i] < self.T]\n",
    "            \n",
    "                filtered_stopping_times.append(filtered_times)\n",
    "            \n",
    "            filtered_stopping_times = np.array(filtered_stopping_times,dtype=object)\n",
    "            \n",
    "        \n",
    "        \n",
    "            return filtered_stopping_times\n",
    "        \n",
    "        \n",
    "        return correlated_brownian_multivariate_sample()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def risk_neutral_probabilities(self,t):\n",
    "        \n",
    "        return 1 - np.exp(-self.intensity * t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def expected_loss(self, t, recovery):\n",
    "        \n",
    "        covariance_matrix = np.array([[1, -np.sqrt(1-self.rho**2)],\n",
    "                                      [-np.sqrt(1-self.rho**2), 1]])\n",
    "        \n",
    "        mean = np.array([0, 0])\n",
    "        \n",
    "        bivariate_distribution = multivariate_normal(mean, covariance_matrix)\n",
    "        \n",
    "        inverse_K_1 = - norm.ppf(self.attachment/(1 - recovery))\n",
    "        \n",
    "        inverse_K_2 = - norm.ppf(self.detachment/(1 - recovery))\n",
    "        \n",
    "        x = np.array([inverse_K_1, norm.ppf(self.risk_neutral_probabilities(t))])\n",
    "        \n",
    "        y = np.array([inverse_K_2, norm.ppf(self.risk_neutral_probabilities(t))])\n",
    "\n",
    "        # Calcul de la perte attendue en fonction de la distribution bivariée\n",
    "        return (bivariate_distribution.cdf(x) - \n",
    "                 bivariate_distribution.cdf(y)) / ((self.detachment - self.attachment)/(1 - recovery))\n",
    "\n",
    "    # Calcul du spread synthétique du CDO en utilisant l'approximation gaussienne\n",
    "    def synthetic_cdo_spread_computation_gaussian_normal(self, proba,recovery):\n",
    "        \n",
    "        spread_list = []\n",
    "    \n",
    "        stopping_times_sim = self.quasi_monte_carlo_stopping_times()\n",
    "        \n",
    "        for i in range(len(stopping_times_sim)):\n",
    "            premium_list = []\n",
    "            protection_list = []\n",
    "\n",
    "            stopping_times_sample = stopping_times_sim[i]\n",
    "\n",
    "            for j in range(len(stopping_times_sample) - 1):\n",
    "                current_time = stopping_times_sample[j]\n",
    "                next_time = stopping_times_sample[j + 1]\n",
    "                current_expected_loss = self.expected_loss(current_time,recovery)\n",
    "                next_expected_loss = self.expected_loss(next_time,recovery)\n",
    "                time_diff = next_time - current_time\n",
    "                discount_factor = np.exp(-self.r * current_time)\n",
    "\n",
    "                # Calcul de la différence des flux de primes entre les deux temps d'arrêt\n",
    "                premium_flow_difference = (next_expected_loss - current_expected_loss) * discount_factor\n",
    "                premium_list.append(premium_flow_difference)\n",
    "\n",
    "                # Calcul de la différence des flux de protection entre les deux temps d'arrêt\n",
    "                protection_flow_difference = time_diff * (1 - current_expected_loss) * discount_factor\n",
    "                protection_list.append(protection_flow_difference)\n",
    "            \n",
    "            epsilon = 1e-10\n",
    "            premium_leg = np.sum(premium_list)\n",
    "            protection_leg = np.sum(protection_list)\n",
    "            \n",
    "            \n",
    "            if protection_leg != 0 and not math.isnan(premium_leg) and not math.isnan(protection_leg):\n",
    "                spread_list.append(premium_leg / protection_leg + epsilon)\n",
    "            \n",
    "            # Calcul de la moyenne, de la variance et de l'intervalle de confiance pour le spread synthétique\n",
    "            mean = np.mean(spread_list)\n",
    "            var = np.var(spread_list, ddof=1)\n",
    "            alpha = 1 - proba \n",
    "            quantile = norm.ppf(1 - alpha / 2)  # fonction quantile \n",
    "            ci_size = quantile * np.sqrt(var / np.array(spread_list).size)\n",
    "        \n",
    "        return pd.DataFrame({'Monte-Carlo Spread': mean,\n",
    "                             'Variance': var,\n",
    "                             'lower_confidence': mean - ci_size,\n",
    "                             'upper_confidence': mean + ci_size}, index=['Halton_Gaussienne']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01ea5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_gaussian = Quasi_Monte_Carlo_Gaussian_CDO(attachment=0.3,detachment=0.7,rho=0.3,intensity=0.2,\n",
    "                              T = 5, d=40,r=0.02,notionel=1,n_samples=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad6d88df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 705 ms, sys: 10.9 ms, total: 716 ms\n",
      "Wall time: 709 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_quasi_gaussien = quasi_gaussian.synthetic_cdo_spread_computation_gaussian_normal(0.95,recovery=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20bffd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Halton_Gaussienne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.122565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.114469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.130660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Halton_Gaussienne\n",
       "Monte-Carlo Spread           0.122565\n",
       "Variance                     0.000495\n",
       "lower_confidence             0.114469\n",
       "upper_confidence             0.130660"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quasi_gaussien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d7534",
   "metadata": {},
   "source": [
    "**Halton Copule de Student a 1 facteur et Approximation LHP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e62ed767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quasi_Monte_Carlo_Student_t_copula:\n",
    "    \n",
    "    def __init__(self, attachment, detachment, rho, intensity, T, d, r, notionel, n_samples, nu):\n",
    "        self.attachment = attachment\n",
    "        self.detachment = detachment\n",
    "        self.rho = rho\n",
    "        self.intensity = intensity\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.r = r\n",
    "        self.notionel = notionel\n",
    "        self.n_samples = n_samples\n",
    "        self.nu = nu\n",
    "    \n",
    "\n",
    "    def quasi_monte_carlo_stopping_times(self):\n",
    "        \n",
    "        def van_der_corput_r(r: int = 1, p: int = 2, scramble: bool = True):\n",
    "            rng = np.random.default_rng() \n",
    "            coeffs = np.arange(p)[np.newaxis]\n",
    "            if scramble:\n",
    "                rng.shuffle(coeffs, axis=1)\n",
    "            seq = coeffs / p\n",
    "            for k in range(2, r+1):\n",
    "                if scramble:\n",
    "                    rng.shuffle(coeffs, axis=1)\n",
    "                seq = (seq + coeffs.T/p**k).reshape((1, -1))\n",
    "            seq = seq.flatten()\n",
    "            return seq\n",
    "\n",
    "        def van_der_corput(n=self.n_samples, p=self.d):\n",
    "            r = np.ceil(np.log(n) / np.log(p))\n",
    "            seq = van_der_corput_r(int(r), p, scramble=True)\n",
    "            return seq[:n]\n",
    "\n",
    "        def halton(n=self.n_samples, d=self.d):\n",
    "            n_primes = primes.first(self.d + 4) \n",
    "            result = [van_der_corput(n, n_primes[k]) for k in range(d)]\n",
    "            return np.array(result).T\n",
    "\n",
    "        def box_muller_multivariate_sample():\n",
    "            quasi_random_samples = halton()\n",
    "            epsilon = 1e-10  \n",
    "            array = np.empty((self.n_samples,self.d))\n",
    "\n",
    "            for i in range(0,int(quasi_random_samples.shape[1]/2)):\n",
    "              \n",
    "                array[:, 2 * i] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.cos(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "                array[:, 2 * i + 1] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.sin(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "            return array\n",
    "\n",
    "        \n",
    "        def correlated_brownian_multivariate_sample():\n",
    "            \n",
    "            multivariate_samples = box_muller_multivariate_sample()\n",
    "            \n",
    "            covariance_matrix = np.full((self.d,self.d),self.rho) + (1 - self.rho) * np.eye(self.d)\n",
    "            \n",
    "            if np.allclose(covariance_matrix,covariance_matrix.T):\n",
    "                try : \n",
    "                    L = np.linalg.cholesky(covariance_matrix)\n",
    "\n",
    "\n",
    "                except np.linalg.LinAlgError : \n",
    "                    print(\"La matrice n'est pas sémi définie positive\")\n",
    "                \n",
    "            # Une fois obtenue L , nous pouvons trouver l'échantillon final correspondant \n",
    "            multivariate_samples_correlated = L @ multivariate_samples.T\n",
    "        \n",
    "            multivariate_samples_correlated = multivariate_samples_correlated.T\n",
    "            \n",
    "            \n",
    "            # simulons les temps d'arret\n",
    "        \n",
    "            stopping_times = (-1/self.intensity) * np.log(norm.cdf(multivariate_samples_correlated))\n",
    "        \n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "            \n",
    "                stopping_times[i] = np.sort(stopping_times[i])\n",
    "            \n",
    "            filtered_stopping_times = []\n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "                filtered_times = stopping_times[i][stopping_times[i] < self.T]\n",
    "            \n",
    "                filtered_stopping_times.append(filtered_times)\n",
    "            \n",
    "            filtered_stopping_times = np.array(filtered_stopping_times,dtype=object)\n",
    "            \n",
    "        \n",
    "        \n",
    "            return filtered_stopping_times\n",
    "        \n",
    "        \n",
    "        return correlated_brownian_multivariate_sample()\n",
    "    \n",
    "    \n",
    "    def risk_neutral_probabilities(self,t):\n",
    "        \n",
    "        return 1 - np.exp(- self.intensity * t)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def student_copula_percentile(self,time,index):\n",
    "        \n",
    "        proba = self.risk_neutral_probabilities(time)\n",
    "        \n",
    "        quantile = t.ppf(proba,self.nu)\n",
    "        \n",
    "        return quantile\n",
    "    \n",
    "    \n",
    "    def expected_loss(self,time,index,recovery=0):\n",
    "        \n",
    "        def f(x):\n",
    "            \n",
    "            return (min(x,(self.detachment/(1 - recovery))) - (self.attachment/(1 - recovery)))\n",
    "        \n",
    "        def g(x):\n",
    "            \n",
    "            c_t = self.student_copula_percentile(time,index)\n",
    "            \n",
    "            return (np.sqrt(1-self.rho**2)/self.rho) * (1/t.pdf(t.ppf(x,self.nu),self.nu)) * t.pdf((np.sqrt(1-self.rho**2) * t.ppf(x,self.nu) - c_t)/self.rho,self.nu)\n",
    "        \n",
    "        def integrand(x):\n",
    "            \n",
    "            return (f(x) * g(x))/((self.detachment - self.attachment)/(1 - recovery))\n",
    "        \n",
    "        \n",
    "        a = (self.attachment/(1 - recovery))\n",
    "        b = 1\n",
    "        \n",
    "        return quad(integrand,a,b)[0]\n",
    "    \n",
    "    \n",
    "    def synthetic_cdo_spread_computation_student(self,proba,recovery=0):\n",
    "        \n",
    "         spread_list = []\n",
    "    \n",
    "         stopping_times_sim = self.quasi_monte_carlo_stopping_times()\n",
    "        \n",
    "         for i in range(len(stopping_times_sim)):\n",
    "            premium_list = []\n",
    "            protection_list = []\n",
    "\n",
    "            stopping_times_sample = stopping_times_sim[i]\n",
    "\n",
    "            for j in range(len(stopping_times_sample) - 1):\n",
    "                current_time = stopping_times_sample[j]\n",
    "                next_time = stopping_times_sample[j + 1]\n",
    "                current_expected_loss = self.expected_loss(current_time,i,recovery)\n",
    "                next_expected_loss = self.expected_loss(next_time,i,recovery)\n",
    "                time_diff = next_time - current_time\n",
    "                discount_factor = np.exp(-self.r * current_time)\n",
    "\n",
    "                premium_flow_difference = (next_expected_loss - current_expected_loss) * discount_factor\n",
    "                premium_list.append(premium_flow_difference)\n",
    "\n",
    "                protection_flow_difference = time_diff * (1 - current_expected_loss) * discount_factor\n",
    "                protection_list.append(protection_flow_difference)\n",
    "            \n",
    "            epsilon = 1e-10\n",
    "            premium_leg = np.sum(premium_list)\n",
    "            protection_leg = np.sum(protection_list)\n",
    "           \n",
    "            if protection_leg != 0 and not math.isnan(premium_leg) and not math.isnan(protection_leg):\n",
    "                spread_list.append(premium_leg / protection_leg + epsilon)\n",
    "            \n",
    "            \n",
    "            mean = np.mean(spread_list)\n",
    "            var = np.var(spread_list, ddof=1)\n",
    "            alpha = 1 - proba \n",
    "            quantile = norm.ppf(1 - alpha/2)  # fonction quantile \n",
    "            ci_size = quantile * np.sqrt(var / np.array(spread_list).size)\n",
    "        \n",
    "         return pd.DataFrame({'Monte-Carlo Spread':mean,\n",
    "                             'Variance':var,\n",
    "                             'lower_confidence':mean - ci_size,\n",
    "                             'upper_confidence':mean + ci_size},index=['Halton_Student']).T\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c759705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_student = Quasi_Monte_Carlo_Student_t_copula(attachment=0.3,detachment=0.7,rho=0.3,intensity=0.2,\n",
    "                              T = 5, d=40,r=0.02,notionel=1,n_samples=10,nu=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77f13bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 179 ms, total: 1min\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_halton_student = quasi_student.synthetic_cdo_spread_computation_student(0.95,recovery=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eac5d6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Halton_Student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.117727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.109420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.126033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Halton_Student\n",
       "Monte-Carlo Spread        0.117727\n",
       "Variance                  0.000180\n",
       "lower_confidence          0.109420\n",
       "upper_confidence          0.126033"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_halton_student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b303845d",
   "metadata": {},
   "source": [
    "**Halton Copule Normale Inverse Gaussienne a 1 facteur et Approximation LHP**\n",
    "\n",
    "\n",
    "Ce code définit une classe Quasi_Monte_Carlo_Normal_Inverse_Gaussian qui permet de simuler un modèle de CDO synthétique basé sur la méthode de Quasi-Monte Carlo et la distribution normale inverse gaussienne (NIG). Voici une description détaillée étape par étape du code :\n",
    "\n",
    "### Quasi_Monte_Carlo_Normal_Inverse_Gaussian :\n",
    "- attachment : point d'attachement du CDO\n",
    "- detachment : point de détachement du CDO\n",
    "- rho : corrélation entre les différents actifs sous-jacents du CDO\n",
    "- intensity : intensité du processus de Poisson pour les temps d'arrêt\n",
    "- T : horizon de temps\n",
    "- d : dimension de l'espace des actifs sous-jacents\n",
    "- r : taux d'intérêt sans risque\n",
    "- notionel : montant notionnel du CDO\n",
    "- n_samples : nombre d'échantillons à utiliser pour les simulations Quasi-Monte Carlo\n",
    "- alpha et beta : paramètres de la distribution normale inverse gaussienne (NIG)\n",
    "\n",
    "\n",
    "### quasi_monte_carlo_stopping_times() \n",
    "\n",
    "est définie pour simuler les temps d'arrêt pour chaque actif sous-jacent en utilisant la méthode Quasi-Monte Carlo. Cette méthode est composée des sous-fonctions suivantes :\n",
    "- van_der_corput_r(): génère une séquence de Van der Corput avec un nombre de termes r, une base p, et une option pour mélanger la séquence (scramble)\n",
    "- van_der_corput(): génère une séquence de Van der Corput avec un nombre de termes n et une base p\n",
    "- halton(): génère une séquence de Halton avec un nombre de termes n et une dimension d\n",
    "- box_muller_multivariate_sample(): génère des échantillons multivariés suivant une distribution normale standard en utilisant la méthode de Box-Muller et la séquence de Halton\n",
    "- correlated_brownian_multivariate_sample(): génère des échantillons multivariés avec une corrélation rho en utilisant la décomposition de Cholesky\n",
    "- La méthode risk_neutral_probabilities() est définie pour calculer les probabilités neutres au risque pour un temps t.\n",
    "\n",
    "\n",
    "### nig_cumulative_distribution(), nig_density_distribution() et inverse_nig_distribution() \n",
    "\n",
    "sont définies pour calculer respectivement la fonction de répartition, la fonction de densité et la fonction de répartition inverse de la distribution normale inverse gaussienne (NIG) pour un argument x et un paramètre de taille s.\n",
    "\n",
    "\n",
    "### nig_expected_loss() \n",
    "\n",
    "est définie pour calculer la perte attendue pour un paramètre a et un temps time.\n",
    "\n",
    "\n",
    "### synthetic_cdo_spread_normal_inverse_gaussian() \n",
    "\n",
    "   - Génération des temps d'arrêt avec la méthode quasi_monte_carlo_stopping_times.\n",
    "\n",
    "   -  Pour chaque échantillon de temps d'arrêt, calcul des flux de prime et des flux de protection en utilisant la perte attendue et les probabilités neutres au risque.\n",
    "\n",
    "   -  Calcul du spread pour chaque échantillon en divisant la somme des flux de prime par la somme des flux de protection. Ajout d'un petit nombre epsilon pour éviter les divisions par zéro.\n",
    "\n",
    "   -  Calcul de la moyenne, de la variance et de l'intervalle de confiance des spreads obtenus à partir des échantillons.\n",
    "\n",
    "   -  Retour des résultats sous forme d'un DataFrame pandas.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "434f262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quasi_Monte_Carlo_Normal_Inverse_Gaussian:\n",
    "    \n",
    "    def __init__(self, attachment, detachment, rho, intensity, T, d, r, notionel, n_samples, alpha, beta):\n",
    "        \n",
    "        self.attachment = attachment\n",
    "        self.detachment = detachment\n",
    "        self.rho = rho\n",
    "        self.intensity = intensity\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.r = r\n",
    "        self.notionel = notionel\n",
    "        self.n_samples = n_samples\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "    def quasi_monte_carlo_stopping_times(self):\n",
    "        \n",
    "        def van_der_corput_r(r: int = 1, p: int = 2, scramble: bool = True):\n",
    "            rng = np.random.default_rng()  # Initialize random number generator\n",
    "            coeffs = np.arange(p)[np.newaxis]\n",
    "            if scramble:\n",
    "                rng.shuffle(coeffs, axis=1)\n",
    "            seq = coeffs / p\n",
    "            for k in range(2, r+1):\n",
    "                if scramble:\n",
    "                    rng.shuffle(coeffs, axis=1)\n",
    "                seq = (seq + coeffs.T/p**k).reshape((1, -1))\n",
    "            seq = seq.flatten()\n",
    "            return seq\n",
    "\n",
    "        def van_der_corput(n=self.n_samples, p=self.d):\n",
    "            r = np.ceil(np.log(n) / np.log(p))\n",
    "            seq = van_der_corput_r(int(r), p, scramble=True)\n",
    "            return seq[:n]\n",
    "\n",
    "        def halton(n=self.n_samples, d=self.d):\n",
    "            n_primes = primes.first(self.d + 4)  # Prime numbers\n",
    "            result = [van_der_corput(n, n_primes[k]) for k in range(d)]\n",
    "            return np.array(result).T\n",
    "\n",
    "        def box_muller_multivariate_sample():\n",
    "            quasi_random_samples = halton()\n",
    "            epsilon = 1e-10  # Small offset to avoid division by zero and invalid values\n",
    "            array = np.empty((self.n_samples,self.d))\n",
    "\n",
    "            for i in range(0,int(quasi_random_samples.shape[1]/2)):\n",
    "              \n",
    "                array[:, 2 * i] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.cos(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "                array[:, 2 * i + 1] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.sin(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "            return array\n",
    "\n",
    "        \n",
    "        def correlated_brownian_multivariate_sample():\n",
    "            \n",
    "            multivariate_samples = box_muller_multivariate_sample()\n",
    "            \n",
    "            covariance_matrix = np.full((self.d,self.d),self.rho) + (1 - self.rho) * np.eye(self.d)\n",
    "            \n",
    "            if np.allclose(covariance_matrix,covariance_matrix.T):\n",
    "                try : \n",
    "                    L = np.linalg.cholesky(covariance_matrix)\n",
    "\n",
    "\n",
    "                except np.linalg.LinAlgError : \n",
    "                    print(\"La matrice n'est pas sémi définie positive\")\n",
    "                \n",
    "            # Une fois obtenue L , nous pouvons trouver l'échantillon final correspondant \n",
    "            multivariate_samples_correlated = L @ multivariate_samples.T\n",
    "        \n",
    "            multivariate_samples_correlated = multivariate_samples_correlated.T\n",
    "            \n",
    "            \n",
    "            # simulons les temps d'arret\n",
    "        \n",
    "            stopping_times = (-1/self.intensity) * np.log(norm.cdf(multivariate_samples_correlated))\n",
    "        \n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "            \n",
    "                stopping_times[i] = np.sort(stopping_times[i])\n",
    "            \n",
    "            filtered_stopping_times = []\n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "                filtered_times = stopping_times[i][stopping_times[i] < self.T]\n",
    "            \n",
    "                filtered_stopping_times.append(filtered_times)\n",
    "            \n",
    "            filtered_stopping_times = np.array(filtered_stopping_times,dtype=object)\n",
    "            \n",
    "        \n",
    "        \n",
    "            return filtered_stopping_times\n",
    "        \n",
    "        \n",
    "        return correlated_brownian_multivariate_sample()\n",
    "    \n",
    "    \n",
    "    def risk_neutral_probabilities(self, t):\n",
    "        \n",
    "        return  1 - np.exp(-self.intensity * t)\n",
    "    \n",
    "    def nig_cumulative_distribution(self, x, s):\n",
    "        \n",
    "        gamma = np.sqrt(self.alpha ** 2 - self.beta ** 2)\n",
    "        alpha, beta = s * self.alpha, s * self.beta\n",
    "        loc = -s * ((-self.beta * gamma ** 2) / (self.alpha ** 2))\n",
    "        scale = s * (gamma ** 3 / (self.alpha ** 2))\n",
    "        \n",
    "        return norminvgauss.cdf(x, alpha * scale, beta * scale, loc, scale)\n",
    "    \n",
    "    def nig_density_distribution(self, x, s):\n",
    "        \n",
    "        gamma = np.sqrt(self.alpha ** 2 - self.beta ** 2)\n",
    "        alpha, beta = s * self.alpha, s * self.beta\n",
    "        loc = -s * ((-self.beta * gamma ** 2) / (self.alpha ** 2))\n",
    "        scale = s * (gamma ** 3 / (self.alpha ** 2))\n",
    "        \n",
    "        return norminvgauss.pdf(x, alpha * scale, beta * scale, loc, scale)\n",
    "    \n",
    "    def inverse_nig_distribution(self, x, s):\n",
    "        gamma = np.sqrt(self.alpha ** 2 - self.beta ** 2)\n",
    "        alpha, beta = s * self.alpha, s * self.beta\n",
    "        loc = -s * ((-self.beta * gamma ** 2) / (self.alpha ** 2))\n",
    "        scale = s * (gamma ** 3 / (self.alpha ** 2))\n",
    "        \n",
    "        return norminvgauss.ppf(x, alpha * scale, beta * scale, loc, scale)\n",
    "    \n",
    "    def nig_expected_loss(self,time, recovery=0):\n",
    "        \n",
    "        f = lambda x: self.nig_cumulative_distribution(x, (np.sqrt(1 - self.rho**2) / self.rho)) - (self.attachment/(1 - recovery))\n",
    "        g = lambda x: self.nig_density_distribution((self.inverse_nig_distribution(self.risk_neutral_probabilities(time), 1 / self.rho) - np.sqrt(1 - self.rho**2) * x) / self.rho, 1) * (np.sqrt(1 - self.rho**2) / self.rho)\n",
    "        f_infinity = lambda x, time: 1 - self.nig_cumulative_distribution(((self.inverse_nig_distribution(self.risk_neutral_probabilities(time), 1 / self.rho) - np.sqrt(1 - self.rho**2) * self.inverse_nig_distribution(x, (np.sqrt(1 - self.rho**2) / self.rho))) / self.rho), 1)\n",
    "        \n",
    "        b = self.inverse_nig_distribution((self.detachment/(1 - recovery)), (np.sqrt(1 - self.rho**2) / self.rho))\n",
    "        z = self.inverse_nig_distribution((self.attachment/(1 - recovery)), (np.sqrt(1 - self.rho**2) / self.rho))\n",
    "        \n",
    "        integrand = lambda x: (f(x) * g(x)) / ((self.detachment - self.attachment)/(1 - recovery))\n",
    "        infinity = f_infinity((self.detachment/(1 - recovery)), time)\n",
    "        \n",
    "        return quad(integrand, z, b)[0] + (1 - infinity)\n",
    "    \n",
    "    def synthetic_cdo_spread_normal_inverse_gaussian(self,proba,recovery=0):\n",
    "    \n",
    "        spread_list = []\n",
    "        stopping_times_sim = self.quasi_monte_carlo_stopping_times()\n",
    "        \n",
    "        for i in range(len(stopping_times_sim)):\n",
    "            premium_list = []\n",
    "            protection_list = []\n",
    "\n",
    "            stopping_times_sample = stopping_times_sim[i]\n",
    "\n",
    "            for j in range(len(stopping_times_sample) - 1):\n",
    "                current_time = stopping_times_sample[j]\n",
    "                next_time = stopping_times_sample[j + 1]\n",
    "                current_expected_loss = self.nig_expected_loss(current_time,recovery)\n",
    "                next_expected_loss = self.nig_expected_loss(next_time,recovery)\n",
    "                time_diff = next_time - current_time\n",
    "                discount_factor = np.exp(-self.r * current_time)\n",
    "\n",
    "                premium_flow_difference = (next_expected_loss - current_expected_loss) * discount_factor\n",
    "                premium_list.append(premium_flow_difference)\n",
    "\n",
    "                protection_flow_difference = time_diff * (1 - current_expected_loss) * discount_factor\n",
    "                protection_list.append(protection_flow_difference)\n",
    "\n",
    "            epsilon = 1e-10    \n",
    "            premium_leg = np.sum(premium_list)\n",
    "            protection_leg = np.sum(protection_list)\n",
    "            \n",
    "            if protection_leg != 0 and not math.isnan(premium_leg) and not math.isnan(protection_leg):\n",
    "                spread_list.append(premium_leg / protection_leg + epsilon)\n",
    "        \n",
    "        mean = np.mean(spread_list)\n",
    "        var = np.var(spread_list, ddof=1)\n",
    "        alpha = 1 - proba\n",
    "        quantile = norm.ppf(1 - alpha / 2)\n",
    "        ci_size = quantile * np.sqrt(var / len(spread_list))\n",
    "    \n",
    "        return pd.DataFrame({'Monte-Carlo Spread': mean,\n",
    "                             'Variance': var,\n",
    "                             'lower_confidence': mean - ci_size,\n",
    "                             'upper_confidence': mean + ci_size}, index=['Halton_Norm.Inv.Gauss.']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6863c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_nig = Quasi_Monte_Carlo_Normal_Inverse_Gaussian(attachment=0.3,detachment=0.7,rho=0.3, intensity=0.2,T=5,\n",
    "                                     d =40, r=0.02,notionel=1, n_samples=10,alpha=1,beta=1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99bb5da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 4s, sys: 6.34 s, total: 8min 10s\n",
      "Wall time: 11min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_quasi_nig = quasi_nig.synthetic_cdo_spread_normal_inverse_gaussian(0.95,recovery=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "967349d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Halton_Norm.Inv.Gauss.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.112722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.123478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Halton_Norm.Inv.Gauss.\n",
       "Monte-Carlo Spread                0.118100\n",
       "Variance                          0.000075\n",
       "lower_confidence                  0.112722\n",
       "upper_confidence                  0.123478"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_quasi_nig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e68a8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Halton_Gaussienne</th>\n",
       "      <th>Halton_Norm.Inv.Gauss.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.122565</td>\n",
       "      <td>0.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.114469</td>\n",
       "      <td>0.112722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.130660</td>\n",
       "      <td>0.123478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Halton_Gaussienne  Halton_Norm.Inv.Gauss.\n",
       "Monte-Carlo Spread           0.122565                0.118100\n",
       "Variance                     0.000495                0.000075\n",
       "lower_confidence             0.114469                0.112722\n",
       "upper_confidence             0.130660                0.123478"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_quasi_gaussien,df_quasi_nig],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78577cc",
   "metadata": {},
   "source": [
    "**Sobol Gaussian Copula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d58435a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quasi_Monte_Carlo_Gaussian_CDO:\n",
    "    \n",
    "    def __init__(self, attachment, detachment, rho, intensity, T, d, r, notionel, n_samples):\n",
    "        \n",
    "        self.attachment = attachment\n",
    "        self.detachment = detachment\n",
    "        self.rho = rho\n",
    "        self.intensity = intensity\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.r = r\n",
    "        self.notionel = notionel\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "    def quasi_monte_carlo_stopping_times(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        def Sobol_sequence():\n",
    "            \n",
    "            return Sobol(self.d,scramble=True,seed=42).random(self.n_samples)\n",
    "        \n",
    "\n",
    "        def box_muller_multivariate_sample():\n",
    "            quasi_random_samples = Sobol_sequence()\n",
    "            epsilon = 1e-10  # Small offset to avoid division by zero and invalid values\n",
    "            array = np.empty((self.n_samples,self.d))\n",
    "\n",
    "            for i in range(0,int(quasi_random_samples.shape[1]/2)):\n",
    "              \n",
    "                array[:, 2 * i] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.cos(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "                array[:, 2 * i + 1] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.sin(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "            return array\n",
    "\n",
    "        \n",
    "        def correlated_brownian_multivariate_sample():\n",
    "            \n",
    "            multivariate_samples = box_muller_multivariate_sample()\n",
    "            \n",
    "            covariance_matrix = np.full((self.d,self.d),self.rho) + (1 - self.rho) * np.eye(self.d)\n",
    "            \n",
    "            if np.allclose(covariance_matrix,covariance_matrix.T):\n",
    "                try : \n",
    "                    L = np.linalg.cholesky(covariance_matrix)\n",
    "\n",
    "\n",
    "                except np.linalg.LinAlgError : \n",
    "                    print(\"La matrice n'est pas sémi définie positive\")\n",
    "                \n",
    "            # Une fois obtenue L , nous pouvons trouver l'échantillon final correspondant \n",
    "            multivariate_samples_correlated = L @ multivariate_samples.T\n",
    "        \n",
    "            multivariate_samples_correlated = multivariate_samples_correlated.T\n",
    "            \n",
    "            \n",
    "            # simulons les temps d'arret\n",
    "        \n",
    "            stopping_times = (-1/self.intensity) * np.log(norm.cdf(multivariate_samples_correlated))\n",
    "        \n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "            \n",
    "                stopping_times[i] = np.sort(stopping_times[i])\n",
    "            \n",
    "            filtered_stopping_times = []\n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "                filtered_times = stopping_times[i][stopping_times[i] < self.T]\n",
    "            \n",
    "                filtered_stopping_times.append(filtered_times)\n",
    "            \n",
    "            filtered_stopping_times = np.array(filtered_stopping_times,dtype=object)\n",
    "            \n",
    "        \n",
    "        \n",
    "            return filtered_stopping_times\n",
    "        \n",
    "        \n",
    "        return correlated_brownian_multivariate_sample()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def risk_neutral_probabilities(self,t):\n",
    "        \n",
    "        return 1 - np.exp(-self.intensity * t)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def expected_loss(self, t, recovery):\n",
    "        \n",
    "        covariance_matrix = np.array([[1, -np.sqrt(1-self.rho**2)],\n",
    "                                      [-np.sqrt(1-self.rho**2), 1]])\n",
    "        \n",
    "        mean = np.array([0, 0])\n",
    "        \n",
    "        bivariate_distribution = multivariate_normal(mean, covariance_matrix)\n",
    "        \n",
    "        inverse_K_1 = - norm.ppf(self.attachment/(1 - recovery))\n",
    "        \n",
    "        inverse_K_2 = - norm.ppf(self.detachment/(1 - recovery))\n",
    "        \n",
    "        x = np.array([inverse_K_1, norm.ppf(self.risk_neutral_probabilities(t))])\n",
    "        \n",
    "        y = np.array([inverse_K_2, norm.ppf(self.risk_neutral_probabilities(t))])\n",
    "\n",
    "        # Calcul de la perte attendue en fonction de la distribution bivariée\n",
    "        return (bivariate_distribution.cdf(x) - \n",
    "                 bivariate_distribution.cdf(y)) / ((self.detachment - self.attachment)/(1 - recovery))\n",
    "\n",
    "    # Calcul du spread synthétique du CDO en utilisant l'approximation gaussienne\n",
    "    def synthetic_cdo_spread_computation_gaussian_normal(self, proba,recovery):\n",
    "        \n",
    "        spread_list = []\n",
    "    \n",
    "        stopping_times_sim = self.quasi_monte_carlo_stopping_times()\n",
    "        \n",
    "        for i in range(len(stopping_times_sim)):\n",
    "            premium_list = []\n",
    "            protection_list = []\n",
    "\n",
    "            stopping_times_sample = stopping_times_sim[i]\n",
    "\n",
    "            for j in range(len(stopping_times_sample) - 1):\n",
    "                current_time = stopping_times_sample[j]\n",
    "                next_time = stopping_times_sample[j + 1]\n",
    "                current_expected_loss = self.expected_loss(current_time,recovery)\n",
    "                next_expected_loss = self.expected_loss(next_time,recovery)\n",
    "                time_diff = next_time - current_time\n",
    "                discount_factor = np.exp(-self.r * current_time)\n",
    "\n",
    "                # Calcul de la différence des flux de primes entre les deux temps d'arrêt\n",
    "                premium_flow_difference = (next_expected_loss - current_expected_loss) * discount_factor\n",
    "                premium_list.append(premium_flow_difference)\n",
    "\n",
    "                # Calcul de la différence des flux de protection entre les deux temps d'arrêt\n",
    "                protection_flow_difference = time_diff * (1 - current_expected_loss) * discount_factor\n",
    "                protection_list.append(protection_flow_difference)\n",
    "            \n",
    "            epsilon = 1e-10\n",
    "            premium_leg = np.sum(premium_list)\n",
    "            protection_leg = np.sum(protection_list)\n",
    "            \n",
    "            \n",
    "            if protection_leg != 0 and not math.isnan(premium_leg) and not math.isnan(protection_leg):\n",
    "                spread_list.append(premium_leg / protection_leg + epsilon)\n",
    "            \n",
    "            # Calcul de la moyenne, de la variance et de l'intervalle de confiance pour le spread synthétique\n",
    "            mean = np.mean(spread_list)\n",
    "            var = np.var(spread_list, ddof=1)\n",
    "            alpha = 1 - proba \n",
    "            quantile = norm.ppf(1 - alpha / 2)  # fonction quantile \n",
    "            ci_size = quantile * np.sqrt(var / np.array(spread_list).size)\n",
    "        \n",
    "        return pd.DataFrame({'Monte-Carlo Spread': mean,\n",
    "                             'Variance': var,\n",
    "                             'lower_confidence': mean - ci_size,\n",
    "                             'upper_confidence': mean + ci_size}, index=['Sobol_Gaussienne']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcb6f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_gaussian = Quasi_Monte_Carlo_Gaussian_CDO(attachment=0.3,detachment=0.7,rho=0.3,intensity=0.2,\n",
    "                              T = 5, d=40,r=0.02,notionel=1,n_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d147c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nacersere/miniforge3/envs/mydeeplearning_env/lib/python3.8/site-packages/scipy/stats/_qmc.py:1382: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  warnings.warn(\"The balance properties of Sobol' points require\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.47 s, sys: 952 ms, total: 6.42 s\n",
      "Wall time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sobol_gaussian = quasi_gaussian.synthetic_cdo_spread_computation_gaussian_normal(0.95,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47f5f6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sobol_Gaussienne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.121976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.119481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.124472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Sobol_Gaussienne\n",
       "Monte-Carlo Spread          0.121976\n",
       "Variance                    0.000323\n",
       "lower_confidence            0.119481\n",
       "upper_confidence            0.124472"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sobol_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a58c3b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Halton_Gaussienne</th>\n",
       "      <th>Halton_Norm.Inv.Gauss.</th>\n",
       "      <th>Halton_Student</th>\n",
       "      <th>Sobol_Gaussienne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.122565</td>\n",
       "      <td>0.118100</td>\n",
       "      <td>0.117727</td>\n",
       "      <td>0.121976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.114469</td>\n",
       "      <td>0.112722</td>\n",
       "      <td>0.109420</td>\n",
       "      <td>0.119481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.130660</td>\n",
       "      <td>0.123478</td>\n",
       "      <td>0.126033</td>\n",
       "      <td>0.124472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Halton_Gaussienne  Halton_Norm.Inv.Gauss.  Halton_Student  \\\n",
       "Monte-Carlo Spread           0.122565                0.118100        0.117727   \n",
       "Variance                     0.000495                0.000075        0.000180   \n",
       "lower_confidence             0.114469                0.112722        0.109420   \n",
       "upper_confidence             0.130660                0.123478        0.126033   \n",
       "\n",
       "                    Sobol_Gaussienne  \n",
       "Monte-Carlo Spread          0.121976  \n",
       "Variance                    0.000323  \n",
       "lower_confidence            0.119481  \n",
       "upper_confidence            0.124472  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_quasi_gaussien,df_quasi_nig,df_halton_student,df_sobol_gaussian],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb381a09",
   "metadata": {},
   "source": [
    "**Sobol Student Copula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06a297c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quasi_Monte_Carlo_Student_t_copula:\n",
    "    \n",
    "    def __init__(self, attachment, detachment, rho, intensity, T, d, r, notionel, n_samples, nu):\n",
    "        self.attachment = attachment\n",
    "        self.detachment = detachment\n",
    "        self.rho = rho\n",
    "        self.intensity = intensity\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.r = r\n",
    "        self.notionel = notionel\n",
    "        self.n_samples = n_samples\n",
    "        self.nu = nu\n",
    "    \n",
    "\n",
    "    def quasi_monte_carlo_stopping_times(self):\n",
    "        \n",
    "        \n",
    "        def Sobol_sequence():\n",
    "            \n",
    "            return Sobol(self.d,scramble=True,seed=42).random(self.n_samples)\n",
    "        \n",
    "\n",
    "        def box_muller_multivariate_sample():\n",
    "            quasi_random_samples = Sobol_sequence()\n",
    "            epsilon = 1e-10  # Small offset to avoid division by zero and invalid values\n",
    "            array = np.empty((self.n_samples,self.d))\n",
    "\n",
    "            for i in range(0,int(quasi_random_samples.shape[1]/2)):\n",
    "              \n",
    "                array[:, 2 * i] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.cos(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "                array[:, 2 * i + 1] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.sin(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "            return array\n",
    "\n",
    "        \n",
    "        def correlated_brownian_multivariate_sample():\n",
    "            \n",
    "            multivariate_samples = box_muller_multivariate_sample()\n",
    "            \n",
    "            covariance_matrix = np.full((self.d,self.d),self.rho) + (1 - self.rho) * np.eye(self.d)\n",
    "            \n",
    "            if np.allclose(covariance_matrix,covariance_matrix.T):\n",
    "                try : \n",
    "                    L = np.linalg.cholesky(covariance_matrix)\n",
    "\n",
    "\n",
    "                except np.linalg.LinAlgError : \n",
    "                    print(\"La matrice n'est pas sémi définie positive\")\n",
    "                \n",
    "            # Une fois obtenue L , nous pouvons trouver l'échantillon final correspondant \n",
    "            multivariate_samples_correlated = L @ multivariate_samples.T\n",
    "        \n",
    "            multivariate_samples_correlated = multivariate_samples_correlated.T\n",
    "            \n",
    "            \n",
    "            # simulons les temps d'arret\n",
    "        \n",
    "            stopping_times = (-1/self.intensity) * np.log(norm.cdf(multivariate_samples_correlated))\n",
    "        \n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "            \n",
    "                stopping_times[i] = np.sort(stopping_times[i])\n",
    "            \n",
    "            filtered_stopping_times = []\n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "                filtered_times = stopping_times[i][stopping_times[i] < self.T]\n",
    "            \n",
    "                filtered_stopping_times.append(filtered_times)\n",
    "            \n",
    "            filtered_stopping_times = np.array(filtered_stopping_times,dtype=object)\n",
    "            \n",
    "        \n",
    "        \n",
    "            return filtered_stopping_times\n",
    "        \n",
    "        \n",
    "        return correlated_brownian_multivariate_sample()\n",
    "    \n",
    "    \n",
    "    def risk_neutral_probabilities(self,t):\n",
    "        \n",
    "        return 1 - np.exp(- self.intensity * t)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def student_copula_percentile(self,time,index):\n",
    "        \n",
    "        proba = self.risk_neutral_probabilities(time)\n",
    "        \n",
    "        quantile = t.ppf(proba,self.nu)\n",
    "        \n",
    "        return quantile\n",
    "    \n",
    "    \n",
    "    def expected_loss(self,time,index,recovery=0):\n",
    "        \n",
    "        def f(x):\n",
    "            \n",
    "            return (min(x,(self.detachment/(1 - recovery))) - (self.attachment/(1 - recovery)))\n",
    "        \n",
    "        def g(x):\n",
    "            \n",
    "            c_t = self.student_copula_percentile(time,index)\n",
    "            \n",
    "            return (np.sqrt(1-self.rho**2)/self.rho) * (1/t.pdf(t.ppf(x,self.nu),self.nu)) * t.pdf((np.sqrt(1-self.rho**2) * t.ppf(x,self.nu) - c_t)/self.rho,self.nu)\n",
    "        \n",
    "        def integrand(x):\n",
    "            \n",
    "            return (f(x) * g(x))/((self.detachment - self.attachment)/(1 - recovery))\n",
    "        \n",
    "        \n",
    "        a = (self.attachment/(1 - recovery))\n",
    "        b = 1\n",
    "        \n",
    "        return quad(integrand,a,b)[0]\n",
    "    \n",
    "    \n",
    "    def synthetic_cdo_spread_computation_student(self,proba,recovery=0):\n",
    "        \n",
    "         spread_list = []\n",
    "    \n",
    "         stopping_times_sim = self.quasi_monte_carlo_stopping_times()\n",
    "        \n",
    "         for i in range(len(stopping_times_sim)):\n",
    "            premium_list = []\n",
    "            protection_list = []\n",
    "\n",
    "            stopping_times_sample = stopping_times_sim[i]\n",
    "\n",
    "            for j in range(len(stopping_times_sample) - 1):\n",
    "                current_time = stopping_times_sample[j]\n",
    "                next_time = stopping_times_sample[j + 1]\n",
    "                current_expected_loss = self.expected_loss(current_time,i,recovery)\n",
    "                next_expected_loss = self.expected_loss(next_time,i,recovery)\n",
    "                time_diff = next_time - current_time\n",
    "                discount_factor = np.exp(-self.r * current_time)\n",
    "\n",
    "                premium_flow_difference = (next_expected_loss - current_expected_loss) * discount_factor\n",
    "                premium_list.append(premium_flow_difference)\n",
    "\n",
    "                protection_flow_difference = time_diff * (1 - current_expected_loss) * discount_factor\n",
    "                protection_list.append(protection_flow_difference)\n",
    "            \n",
    "            epsilon = 1e-10\n",
    "            premium_leg = np.sum(premium_list)\n",
    "            protection_leg = np.sum(protection_list)\n",
    "           \n",
    "            if protection_leg != 0 and not math.isnan(premium_leg) and not math.isnan(protection_leg):\n",
    "                spread_list.append(premium_leg / protection_leg + epsilon)\n",
    "            \n",
    "            \n",
    "            mean = np.mean(spread_list)\n",
    "            var = np.var(spread_list, ddof=1)\n",
    "            alpha = 1 - proba \n",
    "            quantile = norm.ppf(1 - alpha/2)  # fonction quantile \n",
    "            ci_size = quantile * np.sqrt(var / np.array(spread_list).size)\n",
    "        \n",
    "         return pd.DataFrame({'Monte-Carlo Spread':mean,\n",
    "                             'Variance':var,\n",
    "                             'lower_confidence':mean - ci_size,\n",
    "                             'upper_confidence':mean + ci_size},index=['Sobol_Student']).T\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa0f837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_gaussian = Quasi_Monte_Carlo_Student_t_copula(attachment=0.3,detachment=0.7,rho=0.3,intensity=0.2,\n",
    "                              T = 5, d=40,r=0.02,notionel=1,n_samples=10,nu=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7769d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nacersere/miniforge3/envs/mydeeplearning_env/lib/python3.8/site-packages/scipy/stats/_qmc.py:1382: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  warnings.warn(\"The balance properties of Sobol' points require\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 s, sys: 240 ms, total: 53 s\n",
      "Wall time: 52.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_nig_student = quasi_gaussian.synthetic_cdo_spread_computation_student(0.95,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc1d09c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sobol_Student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.127716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.116491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.138942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Sobol_Student\n",
       "Monte-Carlo Spread       0.127716\n",
       "Variance                 0.000328\n",
       "lower_confidence         0.116491\n",
       "upper_confidence         0.138942"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nig_student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdedab61",
   "metadata": {},
   "source": [
    "**Sobol Normal Inverse Gaussian Copula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddef986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quasi_Monte_Carlo_Normal_Inverse_Gaussian:\n",
    "    \n",
    "    def __init__(self, attachment, detachment, rho, intensity, T, d, r, notionel, n_samples, alpha, beta):\n",
    "        \n",
    "        self.attachment = attachment\n",
    "        self.detachment = detachment\n",
    "        self.rho = rho\n",
    "        self.intensity = intensity\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.r = r\n",
    "        self.notionel = notionel\n",
    "        self.n_samples = n_samples\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "    def quasi_monte_carlo_stopping_times(self):\n",
    "        \n",
    "        \n",
    "        def Sobol_sequence():\n",
    "            \n",
    "            return Sobol(self.d,scramble=True,seed=42).random(self.n_samples)\n",
    "        \n",
    "\n",
    "        def box_muller_multivariate_sample():\n",
    "            quasi_random_samples = Sobol_sequence()\n",
    "            epsilon = 1e-10  # Small offset to avoid division by zero and invalid values\n",
    "            array = np.empty((self.n_samples,self.d))\n",
    "\n",
    "            for i in range(0,int(quasi_random_samples.shape[1]/2)):\n",
    "              \n",
    "                array[:, 2 * i] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.cos(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "                array[:, 2 * i + 1] = np.sqrt(-2 * np.log(quasi_random_samples[:, 2 * i] + epsilon)) * np.sin(2 * np.pi * quasi_random_samples[:, 2 * i + 1])\n",
    "\n",
    "            return array\n",
    "\n",
    "        \n",
    "        def correlated_brownian_multivariate_sample():\n",
    "            \n",
    "            multivariate_samples = box_muller_multivariate_sample()\n",
    "            \n",
    "            covariance_matrix = np.full((self.d,self.d),self.rho) + (1 - self.rho) * np.eye(self.d)\n",
    "            \n",
    "            if np.allclose(covariance_matrix,covariance_matrix.T):\n",
    "                try : \n",
    "                    L = np.linalg.cholesky(covariance_matrix)\n",
    "\n",
    "\n",
    "                except np.linalg.LinAlgError : \n",
    "                    print(\"La matrice n'est pas sémi définie positive\")\n",
    "                \n",
    "            # Une fois obtenue L , nous pouvons trouver l'échantillon final correspondant \n",
    "            multivariate_samples_correlated = L @ multivariate_samples.T\n",
    "        \n",
    "            multivariate_samples_correlated = multivariate_samples_correlated.T\n",
    "            \n",
    "            \n",
    "            # simulons les temps d'arret\n",
    "        \n",
    "            stopping_times = (-1/self.intensity) * np.log(norm.cdf(multivariate_samples_correlated))\n",
    "        \n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "            \n",
    "                stopping_times[i] = np.sort(stopping_times[i])\n",
    "            \n",
    "            filtered_stopping_times = []\n",
    "        \n",
    "            for i in range(stopping_times.shape[0]):\n",
    "                filtered_times = stopping_times[i][stopping_times[i] < self.T]\n",
    "            \n",
    "                filtered_stopping_times.append(filtered_times)\n",
    "            \n",
    "            filtered_stopping_times = np.array(filtered_stopping_times,dtype=object)\n",
    "            \n",
    "        \n",
    "        \n",
    "            return filtered_stopping_times\n",
    "        \n",
    "        \n",
    "        return correlated_brownian_multivariate_sample()\n",
    "    \n",
    "    \n",
    "    def risk_neutral_probabilities(self, t):\n",
    "        \n",
    "        return  1 - np.exp(-self.intensity * t)\n",
    "    \n",
    "    def nig_cumulative_distribution(self, x, s):\n",
    "        \n",
    "        gamma = np.sqrt(self.alpha ** 2 - self.beta ** 2)\n",
    "        alpha, beta = s * self.alpha, s * self.beta\n",
    "        loc = -s * ((-self.beta * gamma ** 2) / (self.alpha ** 2))\n",
    "        scale = s * (gamma ** 3 / (self.alpha ** 2))\n",
    "        \n",
    "        return norminvgauss.cdf(x, alpha * scale, beta * scale, loc, scale)\n",
    "    \n",
    "    def nig_density_distribution(self, x, s):\n",
    "        \n",
    "        gamma = np.sqrt(self.alpha ** 2 - self.beta ** 2)\n",
    "        alpha, beta = s * self.alpha, s * self.beta\n",
    "        loc = -s * ((-self.beta * gamma ** 2) / (self.alpha ** 2))\n",
    "        scale = s * (gamma ** 3 / (self.alpha ** 2))\n",
    "        \n",
    "        return norminvgauss.pdf(x, alpha * scale, beta * scale, loc, scale)\n",
    "    \n",
    "    def inverse_nig_distribution(self, x, s):\n",
    "        gamma = np.sqrt(self.alpha ** 2 - self.beta ** 2)\n",
    "        alpha, beta = s * self.alpha, s * self.beta\n",
    "        loc = -s * ((-self.beta * gamma ** 2) / (self.alpha ** 2))\n",
    "        scale = s * (gamma ** 3 / (self.alpha ** 2))\n",
    "        \n",
    "        return norminvgauss.ppf(x, alpha * scale, beta * scale, loc, scale)\n",
    "    \n",
    "    def nig_expected_loss(self,time, recovery=0):\n",
    "        \n",
    "        f = lambda x: self.nig_cumulative_distribution(x, (np.sqrt(1 - self.rho**2) / self.rho)) - (self.attachment/(1 - recovery))\n",
    "        g = lambda x: self.nig_density_distribution((self.inverse_nig_distribution(self.risk_neutral_probabilities(time), 1 / self.rho) - np.sqrt(1 - self.rho**2) * x) / self.rho, 1) * (np.sqrt(1 - self.rho**2) / self.rho)\n",
    "        f_infinity = lambda x, time: 1 - self.nig_cumulative_distribution(((self.inverse_nig_distribution(self.risk_neutral_probabilities(time), 1 / self.rho) - np.sqrt(1 - self.rho**2) * self.inverse_nig_distribution(x, (np.sqrt(1 - self.rho**2) / self.rho))) / self.rho), 1)\n",
    "        \n",
    "        b = self.inverse_nig_distribution((self.detachment/(1 - recovery)), (np.sqrt(1 - self.rho**2) / self.rho))\n",
    "        z = self.inverse_nig_distribution((self.attachment/(1 - recovery)), (np.sqrt(1 - self.rho**2) / self.rho))\n",
    "        \n",
    "        integrand = lambda x: (f(x) * g(x)) / ((self.detachment - self.attachment)/(1 - recovery))\n",
    "        infinity = f_infinity((self.detachment/(1 - recovery)), time)\n",
    "        \n",
    "        return quad(integrand, z, b)[0] + (1 - infinity)\n",
    "    \n",
    "    def synthetic_cdo_spread_normal_inverse_gaussian(self,proba,recovery=0):\n",
    "    \n",
    "        spread_list = []\n",
    "        stopping_times_sim = self.quasi_monte_carlo_stopping_times()\n",
    "        \n",
    "        for i in range(len(stopping_times_sim)):\n",
    "            premium_list = []\n",
    "            protection_list = []\n",
    "\n",
    "            stopping_times_sample = stopping_times_sim[i]\n",
    "\n",
    "            for j in range(len(stopping_times_sample) - 1):\n",
    "                current_time = stopping_times_sample[j]\n",
    "                next_time = stopping_times_sample[j + 1]\n",
    "                current_expected_loss = self.nig_expected_loss(current_time,recovery)\n",
    "                next_expected_loss = self.nig_expected_loss(next_time,recovery)\n",
    "                time_diff = next_time - current_time\n",
    "                discount_factor = np.exp(-self.r * current_time)\n",
    "\n",
    "                premium_flow_difference = (next_expected_loss - current_expected_loss) * discount_factor\n",
    "                premium_list.append(premium_flow_difference)\n",
    "\n",
    "                protection_flow_difference = time_diff * (1 - current_expected_loss) * discount_factor\n",
    "                protection_list.append(protection_flow_difference)\n",
    "\n",
    "            epsilon = 1e-10    \n",
    "            premium_leg = np.sum(premium_list)\n",
    "            protection_leg = np.sum(protection_list)\n",
    "            \n",
    "            if protection_leg != 0 and not math.isnan(premium_leg) and not math.isnan(protection_leg):\n",
    "                spread_list.append(premium_leg / protection_leg + epsilon)\n",
    "        \n",
    "        mean = np.mean(spread_list)\n",
    "        var = np.var(spread_list, ddof=1)\n",
    "        alpha = 1 - proba\n",
    "        quantile = norm.ppf(1 - alpha / 2)\n",
    "        ci_size = quantile * np.sqrt(var / len(spread_list))\n",
    "    \n",
    "        return pd.DataFrame({'Monte-Carlo Spread': mean,\n",
    "                             'Variance': var,\n",
    "                             'lower_confidence': mean - ci_size,\n",
    "                             'upper_confidence': mean + ci_size}, index=['Sobol_Norm.Inv.Gauss.']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6796bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_nig = Quasi_Monte_Carlo_Normal_Inverse_Gaussian(attachment=0.3,detachment=0.7,rho=0.3, intensity=0.2,T=5,\n",
    "                                     d = 40, r=0.02,notionel=1, n_samples=5, alpha=1,beta=0.625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "778eaa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nacersere/miniforge3/envs/mydeeplearning_env/lib/python3.8/site-packages/scipy/stats/_qmc.py:1382: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  warnings.warn(\"The balance properties of Sobol' points require\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 2.31 s, total: 3min 1s\n",
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sobol_nig = quasi_nig.synthetic_cdo_spread_normal_inverse_gaussian(0.95,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee5830bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sobol_Norm.Inv.Gauss.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.130198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.113769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.146627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Sobol_Norm.Inv.Gauss.\n",
       "Monte-Carlo Spread               0.130198\n",
       "Variance                         0.000351\n",
       "lower_confidence                 0.113769\n",
       "upper_confidence                 0.146627"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sobol_nig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "463d57d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Halton_Gaussienne</th>\n",
       "      <th>Halton_Norm.Inv.Gauss.</th>\n",
       "      <th>Halton_Student</th>\n",
       "      <th>Sobol_Gaussienne</th>\n",
       "      <th>Sobol_Norm.Inv.Gauss.</th>\n",
       "      <th>Sobol_Student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.122565</td>\n",
       "      <td>0.118100</td>\n",
       "      <td>0.117727</td>\n",
       "      <td>0.121976</td>\n",
       "      <td>0.130198</td>\n",
       "      <td>0.127716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.114469</td>\n",
       "      <td>0.112722</td>\n",
       "      <td>0.109420</td>\n",
       "      <td>0.119481</td>\n",
       "      <td>0.113769</td>\n",
       "      <td>0.116491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.130660</td>\n",
       "      <td>0.123478</td>\n",
       "      <td>0.126033</td>\n",
       "      <td>0.124472</td>\n",
       "      <td>0.146627</td>\n",
       "      <td>0.138942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Halton_Gaussienne  Halton_Norm.Inv.Gauss.  Halton_Student  \\\n",
       "Monte-Carlo Spread           0.122565                0.118100        0.117727   \n",
       "Variance                     0.000495                0.000075        0.000180   \n",
       "lower_confidence             0.114469                0.112722        0.109420   \n",
       "upper_confidence             0.130660                0.123478        0.126033   \n",
       "\n",
       "                    Sobol_Gaussienne  Sobol_Norm.Inv.Gauss.  Sobol_Student  \n",
       "Monte-Carlo Spread          0.121976               0.130198       0.127716  \n",
       "Variance                    0.000323               0.000351       0.000328  \n",
       "lower_confidence            0.119481               0.113769       0.116491  \n",
       "upper_confidence            0.124472               0.146627       0.138942  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_quasi_gaussien,df_quasi_nig,df_halton_student,df_sobol_gaussian,df_sobol_nig,df_nig_student],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af664308",
   "metadata": {},
   "source": [
    "**Séquence de Kakutani**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4f817",
   "metadata": {},
   "source": [
    "Soit $p_1, p_2, \\dots, p_d$ des nombres premiers distincts, et $y_1, \\dots, y_d \\in [0, 1[$ avec $y_i > \\frac{1}{p_i}$. La suite de Kakutani $d$-dimensionnelle $(\\xi_n)_{n\\geq 1}$ est définie par\n",
    "$$\n",
    "\\xi_{n+1} = \\xi_n \\oplus_p y \\quad \\text{en vectoriel},\n",
    "$$\n",
    "avec $\\oplus_p$ une addition avec propagation de la retenue de gauche à droite dans l'écriture en base $p$. Par exemple, $0.120482 \\oplus_{10} 0.340851 = 0.460243$.\n",
    "\n",
    "La famille de séquences a été obtenue pour la première fois en tant que sous-produit lors de la tentative de génération de la séquence de Halton en tant qu'orbite d'une transformation ergodique. Cette extension est basée sur l'addition p-adique sur $[0, 1]$, avec $p$ entier, $p \\geq 2$. Elle est également connue sous le nom de machine à addition de Kakutani. Elle est définie sur les expansions p-adiques régulières des nombres réels de $[0, 1]$ comme l'addition de gauche à droite des expansions p-adiques régulières avec report. L'expansion p-adique régulière de 1 est conventionnellement fixée à $1 = 0.(p − 1)(p − 1)(p − 1)...p$ et $1$ n'est pas considéré comme un nombre rationnel p-adique dans le reste de cette section.\n",
    "Soit $\\oplus_p$ cette addition (ou machine à addition de Kakutani). Ainsi, par exemple, $0.123333... \\oplus_{10} 0.412777... = 0.535011...$\n",
    "\n",
    "\n",
    "De manière plus formelle, si $x, y \\in [0, 1]$ avec leurs expansions p-adiques régulières respectives\n",
    "$0,x_1x_2\\cdots x_k\\cdots$ et $0,y_1y_2\\cdots y_k\\cdots$, alors $x \\oplus_p y$ est défini par $x \\oplus_p y = \\sum_{k \\geq 1} z_k p^{-k}$,\n",
    "où la séquence $\\{0, \\ldots, p - 1\\}$-valuée $(z_k)_{k \\geq 1}$ est donnée par $z_k = (x_k + y_k + \\varepsilon_{k - 1}) \\mod p$ et $\\varepsilon_k = 1_{\\{x_k + y_k + \\varepsilon_{k - 1} \\geq p\\}}$, $k \\geq 1$,\n",
    "avec $\\varepsilon_0 = 0$.\n",
    "\n",
    "   - Si $x$ ou $y$ est un nombre rationnel p-adique et $x, y \\neq 1$, alors on vérifie facilement que $z_k = y_k$ ou $z_k = y_k$ pour chaque $k$ suffisamment grand, de sorte que cela définit une expansion régulière, c'est-à-dire les chiffres $(x \\oplus_p y)_k$ de $x \\oplus_p y$ sont $(x \\oplus_p y)_k = z_k, k \\geq 1$.\n",
    "   - Si $x$ et $y$ ne sont pas des nombres rationnels p-adiques, il se peut que $z_k = p - 1$ pour chaque $k$ suffisamment grand, de sorte que $\\sum_{k \\geq 1} z_k p^{-k}$ n'est pas l'expansion p-adique régulière de $x \\oplus_p y$. C'est le cas, par exemple, dans la pseudo-somme suivante :\n",
    "    $0.123333... \\oplus_{10} 0.412666... = 0.535999... = 0.536$\n",
    "    où $(x \\oplus_p y)_1 = 5$, $(x \\oplus_p y)_2 = 3$, $(x \\oplus_p y)_3 = 5$ et $(x \\oplus_p y)_k = 9$, $k \\geq 4$. Alors, pour chaque $y \\in [0, 1]$, on définit la rotation p-adique associée avec angle $y$ par\n",
    "    $T_{p, y}(x) := x \\oplus_p y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fbdc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kakutani_addition(x, y, p):\n",
    "    carry, z_digits = 0, []\n",
    "    for x_k, y_k in zip(x[2:], y[2:]):\n",
    "        x_k, y_k = int(x_k), int(y_k)\n",
    "        z_k = (x_k + y_k + carry) % p\n",
    "        carry = 1 if x_k + y_k + carry >= p else 0\n",
    "        z_digits.append(z_k)\n",
    "    return '0.' + ''.join(map(str, z_digits))\n",
    "\n",
    "def generate_kakutani_sequence(p, y, n):\n",
    "    sequence = []\n",
    "    x = '0.' + '0' * (n - 2)\n",
    "    for _ in range(n):\n",
    "        x = kakutani_addition(x, y, p)\n",
    "        sequence.append(float(x))\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bc379c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.412, 0.824, 0.246, 0.658, 0.07]\n"
     ]
    }
   ],
   "source": [
    "sequence = generate_kakutani_sequence(10, \"0.412777\", 5)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf283fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

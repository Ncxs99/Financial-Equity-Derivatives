{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436e9379",
   "metadata": {},
   "source": [
    "# MONTE CARLO CDO TRANCHE PRICING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb729b",
   "metadata": {},
   "source": [
    "Dans ce notebook, nous allons essayer de pricer, avec plusieurs différentes méthodes en **monte carlo**, des tranches de CDO synthétiques. L'idée centrale consiste a comparer plusieurs différentes méthodes entre elles pour juger de la qualité, des forces et faiblesses de chacune d'entre elles. Parmi les différentes méthodes, nous aurons entre autres : \n",
    "\n",
    "- La méthode de la copule gaussienne \n",
    "\n",
    "- La méthode de la copule de Student \n",
    "\n",
    "- La méthode de la copule de la fonction de répartition de la normale inverse gaussienne\n",
    "\n",
    "\n",
    "Mais avant de commencer a implémenter les méthodes citées plus haut, il est important d'importer les librairies nécessaires et indispensables au calcul scientifique en Python ( Scipy )  et celui indispensable au calcul optimisé des tableaux ( Numpy ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d748001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import t\n",
    "from scipy.stats import norm, norminvgauss\n",
    "from scipy.integrate import quad\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"numpy.core.fromnumeric\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"numpy.core._methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a0d4a2",
   "metadata": {},
   "source": [
    "**Méthode de la copule Gaussienne a un facteur et Approximation LHP**\n",
    "\n",
    "La méthode de la copule gaussienne est un des modèles mathématiques les plus utilisés en finance pour déterminer le prix des tranches de CDO synthétiques. Le principe est le suivant : \n",
    "\n",
    "\n",
    "Le modèle LHP est basé sur une copule gaussienne à un facteur de défauts corrélés. Supposons que le portefeuille d'actifs de référence se compose de $m$ instruments financiers et que le rendement de l'actif jusqu'au temps $t$ du $i$-ème émetteur du portefeuille, $A_i(t)$, ait la forme :\n",
    "\\begin{equation}\n",
    "A_i(t) = a_i M(t) + \\sqrt{1 - a_i^2} X_i(t), \\qquad \n",
    "\\end{equation}\n",
    "où $M(t)$, $X_i(t)$, $i = 1, \\dots, m$, sont des variables aléatoires indépendantes suivant une distribution normale standard. Alors, conditionnellement au facteur de marché commun $M(t)$, les rendements des actifs des différents émetteurs sont indépendants. On remarque que, en raison de la stabilité des distributions normales sous convolution, le rendement de l'actif $A_i(t)$ suit également une distribution normale standard.\n",
    "\n",
    "Dans ce modèle de copule, la variable $A_i(t)$ est transformée en temps de défaut $t_i$ du $i$-ème émetteur en utilisant une transformation percentile-à-percentile, c'est-à-dire\n",
    "\\begin{equation}\n",
    "\\mathbb{P}[\\tau_i \\leq t] = \\mathbb{P}[A_i(t) \\leq C_i(t)]. \\qquad \n",
    "\\end{equation}\n",
    "Nous notons la probabilité pour l'émetteur $i$ de faire défaut avant le temps $t$ par\n",
    "\\begin{equation}\n",
    "q_i(t) = \\mathbb{P} [\\tau_i \\leq t]. \\qquad \n",
    "\\end{equation}\n",
    "Les probabilités neutres au risque sont déduites des prix du marché observables des instruments de défaut de crédit (par exemple, les obligations ou CDS).\n",
    "\n",
    "Alors les seuils $C_i(t)$ peuvent être calculés comme suit :\n",
    "\\begin{equation}\n",
    "C_i(t) = \\Phi^{-1}(q_i(t)),\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Dans notre cas, la probabilité risque neutre a été calculée avec un modèle a intensité. Autrement dit nous avons considéré que le temps d'arrèt ( temps auquel le défaut interviendra suit un processus de poisson dont on a supposé l'intensité constante ). En réalité, de manière rigoureuse, l'intensité de défaut doit etre calculé avec ce que l'on appelle **le triangle de crédit**. Il s'agit d'une relation mathématique qui lie le spread de crédit de chaque actif constitutifs du portefeuilles de crédits, le taux de recouvrement et l'intensité $\\lambda$ du défaut. Dans notre cas, a défaut d'avoir le spread de crédit de chaque actif présent sur le marché, nous avons choisi une valeur arbitraire de l'intensité comme input de notre modèle ( 0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f780d42",
   "metadata": {},
   "source": [
    "Voici comment nous avons procédé dans notre modélisation : **Pricing des CDO synthétiques avec des temps de défauts**\n",
    "\n",
    "Une simulation de Monte Carlo des temps de défaut nous permet d'évaluer tout dérivé de crédit dont le prix est une fonction de l'identité et du moment des défauts sur le portefeuille de référence. De tels produits comprennent les principaux produits de corrélation c'est-à-dire les paniers de défaut et la plupart des variations sur le CDO.\n",
    "Le principal défi lors de la tarification des dérivés de crédit à l'aide de Monte Carlo est la minimisation de l'erreur standard de l'estimation du prix. Étant donné que l'erreur standard ne diminue que comme $\\mathcal{O}(1/\\sqrt{n})$ et que le temps de Monte Carlo évolue comme $\\mathcal{O}(n\\cdot P)$, nous devons nous assurer que le code s'exécute le plus rapidement possible. Cela nécessite une combinaison de conception de code soignée et de l'utilisation de méthodes de réduction de variance. Des méthodes telles que les variables de contrôle, l'échantillonnage d'importance et les générateurs de nombres pseudo-aléatoires peuvent tous être utilisés.En ce qui concerne la conception de code, nous pouvons diviser la tarification de Monte Carlo des dérivés de crédit en deux approches :\n",
    "\n",
    "- Nous utilisons les temps de défaut pour calculer directement la valeur actualisée de chaque flux de la jambe de prime et de protection dans chaque scénario, puis nous faisons la moyenne de ces valeurs pour calculer l'espérance actualisée de chaque jambe.\n",
    "\n",
    "\n",
    "- Nous utilisons les temps de défaut pour calculer la courbe de survie de la tranche, que nous fournissons ensuite à nos outils d'analyse de CDS pour calculer la valeur des différentes jambes. C'est l'approche indirecte. Cela n'est bien sûr possible que si le produit peut être évalué en utilisant simplement une courbe de survie, c'est-à-dire s'il inclut des STCDO ( Single Tranche CDO Pricing ) et des paniers de défaut à perte homogène.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Utiliser l'approche directe nécessite d'écrire un algorithme qui évalue les jambes de prime et de protection étant donné un vecteur de N temps de défaut générés aléatoirement pour chacun des P essais de Monte Carlo. Nous notons ce vecteur avec $\\{ \\tau_p^i \\}$ et supposons que ces temps de défaut ont déjà été générés. Les étapes sont les suivantes :\n",
    "\n",
    "- Nous définissons $p=1$.\n",
    "- Nous supprimons les temps de défaut $\\tau_p^i > T$ où $T$ est la maturité finale du contrat.\n",
    "- Nous ordonnons les temps de défaut $\\tau_p^i$ par ordre croissant en utilisant un algorithme de tri efficace tel que QuickSort, comme décrit dans Press et al. (1992).\n",
    "- Nous parcourons les temps de défaut de tous les crédits qui font défaut avant le temps $T$ et calculons les flux sur les jambes de protection et de paiement. Nous connaissons l'identité des crédits en défaut, nous pouvons donc attribuer le taux de recouvrement approprié.\n",
    "- Nous actualisons les flux sur la jambe principale et de protection en utilisant la courbe Libor pour donner la valeur actualisée de la jambe de protection $PV_p$ et $RPV01_p$.\n",
    "- Nous définissons $p=p+1$. Si $p \\leq P$, nous revenons à l'étape (2).\n",
    "- Nous calculons la moyenne des valeurs actualisées de la jambe de prime et de protection:\n",
    "\n",
    "$$ Tranche Risky PV01 = \\frac{1}{P} \\sum_{p = 1}^{P} RPVO1_p $$ et la moyenne de la jambe de protection est : \n",
    "\n",
    "$$ Tranche Protection leg PV = \\sum_{p = 1}^{P} Protection leg PV_p $$\n",
    "\n",
    "\n",
    "L'avantage de cette approche est qu'elle est très générique et peut gérer n'importe quel dérivé de crédit dont les paiements dépendent des temps de défaut et des pertes associées à chaque crédit. Par exemple, elle peut gérer les paniers de défaut de n-ième à défaut où les pertes sont inhomogènes. Bien qu'elle fasse de nombreux appels à la fonction de facteur d'actualisation Libor, cela peut être accéléré en mettant en cache la courbe de facteur d'actualisation. \n",
    "\n",
    "\n",
    "Comme décrit précédemment, nous avons suivi les memes étapes pour pricer les CDO synthétiques dans le cadre de la copule gaussienne. Pour le confort du lecteur, nous rappelons les différentes étapes pour le cas de la **copule gaussienne** : \n",
    "\n",
    "Rappelons succinctement comment on obtient des tirages des temps d'arrèt.\n",
    "On réalise d'abord un tirage $u = (u_1, \\dots, u_n)$ d'une loi normale $N(\\cdot; \\rho)$ M-dimensionnelle à composantes centrées réduites et corrélées selon une matrice de corrélation $[\\rho]$ donnée. Cette dernière est choisie en fonction de la corrélation présumée des événements de crédit. Un tel tirage peut faire appel à une décomposition de Choleski, conformément à la méthode exposée dans le cour de monte carlo. Une méthode simple pour déterminer les corrélations et effectuer les tirages est de s'appuyer sur un modèle mono-factoriel : $U_i = \\rho_i L + \\sqrt{1 - \\rho_i^2} \\varepsilon_i$, où $U_i, L$ et $\\varepsilon_i$ sont des gaussiennes standardisées et $\\varepsilon_i$ est indépendante de $L$ et de $\\varepsilon_j$ pour $i \\neq j$; $\\rho_i$ est la corrélation de $U_i$ avec la conjoncture globale $L$ et $\\text{corr}(U_i, U_j) = \\rho_i \\rho_j$. À ce tirage gaussien $(u_1, \\dots, u_n)$, on fait correspondre un vecteur de M dates de défaut par la relation:\n",
    "\\begin{equation}\n",
    "  \\quad T_i^{(p)} = (\\Phi^{-1}(N(u_1)), \\dots, \\Phi^{-1}(N(u_n))) \n",
    "\\end{equation}\n",
    "Quand les probabilités de défaut sont déterminées à l'aide d'un modèle à intensité, l'équation précédente donne : $$T_i^{(p)} = (-\\frac{1}{\\lambda_i} \\ln(N(u_i)), \\dots, -\\frac{1}{\\lambda_n} \\ln(N(u_n)))$$ \n",
    "Ce vecteur $T^{(p)}$ constitue un élément d'une simulation des M dates de défaut, présumé tiré de la loi jointe des $T_i$. On remarquera que la loi marginale de la i-ième composante est bien $\\Phi^{-1}$ et que les différentes composantes sont liées par la corrélation de copule $[\\rho]$.\n",
    "\n",
    "En répétant le tirage gaussien p fois et en appliquant à chaque tirage la seconde relation des temps d'arrèts , on construit un échantillon de p éléments M-dimensionnels $(T^{(1)}, \\dots, T^{(p)})$ constituant une simulation de Monte Carlo des dates de défaut. \n",
    "\n",
    "\n",
    "\n",
    "Pour résumer, voici ce que fait notre code étape par étape : \n",
    "\n",
    "\n",
    "# Classe GaussianCDOPricing\n",
    "\n",
    "Cette classe simule le pricing d'un CDO synthétique en utilisant la copule gaussienne.\n",
    "\n",
    "### \\_\\_init\\_\\_\n",
    "\n",
    "Initialise les attributs de la classe :\n",
    "\n",
    "- attachment : point d'attachement\n",
    "- detachment : point de détachement\n",
    "- rho : corrélation\n",
    "- intensity : intensité de défaut\n",
    "- T : maturité du CDO\n",
    "- d : nombre d'actifs du CDO\n",
    "- r : taux d'intérêt constant\n",
    "- notionel : nominal\n",
    "- n_samples : nombre de simulations\n",
    "\n",
    "### stopping_times_simulation\n",
    "\n",
    "Simule les temps d'arrêt en utilisant la copule gaussienne :\n",
    "\n",
    "- Initialise la matrice de corrélation et génère des échantillons multivariés gaussiens.\n",
    "- Utilise la méthode antithétique pour améliorer l'efficacité de la simulation.\n",
    "- Corrèle les échantillons multivariés en utilisant la décomposition de Cholesky.\n",
    "- Calcule les temps d'arrêt en utilisant la fonction de répartition inverse de la distribution gaussienne standard.\n",
    "- Filtre les temps d'arrêt pour ne garder que ceux qui sont inférieurs à la maturité du CDO.\n",
    "- Retourne un tableau numpy des temps d'arrêt filtrés.\n",
    "\n",
    "### risk_neutral_probabilities\n",
    "\n",
    "Calcule les probabilités neutres au risque en utilisant l'intensité de défaut.\n",
    "\n",
    "### expected_loss\n",
    "\n",
    "Calcule la perte attendue à un instant donné :\n",
    "\n",
    "- Initialise la matrice de covariance bivariée et la distribution gaussienne bivariée.\n",
    "- Calcule les quantiles inverses des points d'attachement et de détachement.\n",
    "- Calcule la perte attendue en utilisant la fonction de répartition de la distribution bivariée et les quantiles inverses.\n",
    "\n",
    "L'expected loss est donnée par : \n",
    "\n",
    "$$E L_{\\left(K_1, K_2\\right)}(t)=\\frac{\\Phi_2\\left(-\\Phi^{-1}\\left(K_1\\right), C(t), \\rho\\right)-\\Phi_2\\left(-\\Phi^{-1}\\left(K_2\\right), C(t), \\rho\\right)}{K_2-K_1}$$ avec $\\rho$ la matrice de corrélation $$[[1, -\\sqrt{1 - a^2}], \\\\\n",
    "[-\\sqrt{1 - a^2}, 1]]$$, avec $C(t)$ donnée par la formule \n",
    "\\begin{equation}\n",
    "C(t) = \\Phi^{-1}(q(t)), \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "avec $q(t)$\n",
    "\n",
    "\\begin{equation}\n",
    "q(t) = 1 - \\exp(-\\lambda * t)\n",
    "\\end{equation}\n",
    "\n",
    "ou $\\lambda$ est l'intensité de défaut. Cela est possible puisque la loi normale étant stable par convolution, les \n",
    "$A_i(t)$ suivent bien une loi normale centrée et réduite. Le portefeuille étant homogène, tous les actifs ont les memes composantes.\n",
    "\n",
    "Comme nous le voyons avec une copule gaussienne, nous voyons que l'expected loss a une expression fermée. Nous avons eu a l'implémenter directement sur Python. Nous disposions de toutes les variables qui sont des données du problèmes. \n",
    "\n",
    "### synthetic_cdo_spread_computation_gaussian_normal\n",
    "\n",
    "Calcule le spread du CDO synthétique en utilisant la méthode de Monte-Carlo :\n",
    "\n",
    "- Initialise une liste pour stocker les spreads.\n",
    "- Pour chaque échantillon de temps d'arrêt simulé :\n",
    "  - Initialise des listes pour stocker les flux de primes et de protection.\n",
    "  - Pour chaque paire de temps d'arrêt adjacents :\n",
    "    - Calcule les pertes attendues aux temps d'arrêt courant et suivant en considérant le taux de recouvrement.\n",
    "    - Calcule la différence de temps entre les temps d'arrêt courant et suivant.\n",
    "    - Calcule le facteur d'actualisation en utilisant le taux d'intérêt constant $r$ et le temps d'arrêt courant.\n",
    "    - Calcule la différence des flux de primes et de protection en utilisant les pertes attendues, la différence de temps et le facteur d'actualisation.\n",
    "    - Ajoute les différences de flux de primes et de protection aux listes correspondantes.\n",
    "  - Calcule la jambe de prime et la jambe de protection en sommant les flux de primes et de protection, respectivement.\n",
    "  - Calcule le spread en divisant la jambe de prime par la jambe de protection, et ajoute le résultat à la liste des spreads.\n",
    "- Calcule la moyenne, la variance et l'intervalle de confiance des spreads.\n",
    "\n",
    "- Retourne un DataFrame pandas contenant les résultats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae77ba3",
   "metadata": {},
   "source": [
    "**Taux de Recouvrement en cas de défaut**\n",
    "\n",
    "Le taux de recouvrement est le pourcentage du nominal que l'on espère percevoir en cas de défaut. Vu que nous travaillons avec des tranches de CDO synthétiques et que $K_1$ et $K_2$ , qui sont respectivement les points d'attachements et de détachements sont **compris entre 0 et 1** dans l'article, le taux de recouvrement dénoté $R$ doit respecter certaines contraintes mathématiques majeures sous peines de ne pas pouvoir calculer les quantiles associés : \n",
    "\n",
    "$$R > 0$$ et $$R < 1 - K_2$$ Par conséquent, $$0 < R < 1 - K_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c111a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianCDOPricing:\n",
    "    \n",
    "    def __init__(self, attachment, detachment, rho, intensity, T, d, r, notionel, n_samples):\n",
    "        \n",
    "        self.attachment = attachment   # point d'attachement (limite inférieure de la tranche de CDO)\n",
    "        self.detachment = detachment   # point de détachement (limite supérieure de la tranche de CDO)\n",
    "        self.rho = rho  # corrélation entre les actifs du CDO\n",
    "        self.intensity = intensity   # intensité de défaut des actifs\n",
    "        self.T = T   # maturité du CDO\n",
    "        self.d = d   # nombre d'actifs du CDO\n",
    "        self.r = r   # taux d'intérêt constant\n",
    "        self.notionel = notionel    # nominal du CDO\n",
    "        self.n_samples = n_samples  # nombre de simulations pour la méthode Monte Carlo\n",
    "\n",
    "    # Simulation des temps d'arrêt (défauts) pour les actifs du CDO\n",
    "    def stopping_times_simulation(self):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        mean = np.repeat(0, self.d)\n",
    "        covariance_matrix = np.eye(self.d)\n",
    "\n",
    "        half_n_samples = self.n_samples // 2\n",
    "        multivariate_samples = np.random.multivariate_normal(mean, covariance_matrix, half_n_samples)\n",
    "        antithetic_samples = -multivariate_samples\n",
    "\n",
    "        multivariate_samples = np.vstack((multivariate_samples, antithetic_samples))\n",
    "\n",
    "        covariance_matrix = np.full((self.d, self.d), self.rho) + (1 - self.rho) * np.eye(self.d)\n",
    "\n",
    "        if np.allclose(covariance_matrix, covariance_matrix.T):\n",
    "            try:\n",
    "                L = np.linalg.cholesky(covariance_matrix)\n",
    "\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"La matrice n'est pas sémi définie positive\")\n",
    "\n",
    "        multivariate_samples_correlated = L @ multivariate_samples.T\n",
    "        multivariate_samples_correlated = multivariate_samples_correlated.T\n",
    "\n",
    "        stopping_times = (-1 / self.intensity) * np.log(norm.cdf(multivariate_samples_correlated))\n",
    "\n",
    "        for i in range(stopping_times.shape[0]):\n",
    "            stopping_times[i] = np.sort(stopping_times[i])\n",
    "\n",
    "        filtered_stopping_times = []\n",
    "\n",
    "        for i in range(stopping_times.shape[0]):\n",
    "            filtered_times = stopping_times[i][stopping_times[i] < self.T]\n",
    "            filtered_stopping_times.append(filtered_times)\n",
    "\n",
    "        filtered_stopping_times = np.array(filtered_stopping_times, dtype=object)\n",
    "\n",
    "        return filtered_stopping_times\n",
    "\n",
    "    # Calcul des probabilités neutres au risque pour un temps donné\n",
    "    def risk_neutral_probabilities(self, t):\n",
    "        \n",
    "        return 1 - np.exp(-self.intensity * t)\n",
    "    \n",
    "    # Calcul de la perte attendue pour un temps donné\n",
    "    def expected_loss(self, t, recovery):\n",
    "        \n",
    "        covariance_matrix = np.array([[1, -np.sqrt(1-self.rho**2)],\n",
    "                                      [-np.sqrt(1-self.rho**2), 1]])\n",
    "        \n",
    "        mean = np.array([0, 0])\n",
    "        \n",
    "        bivariate_distribution = multivariate_normal(mean, covariance_matrix)\n",
    "        \n",
    "        inverse_K_1 = - norm.ppf(self.attachment/(1 - recovery))\n",
    "        \n",
    "        inverse_K_2 = - norm.ppf(self.detachment/(1 - recovery))\n",
    "        \n",
    "        x = np.array([inverse_K_1, norm.ppf(self.risk_neutral_probabilities(t))])\n",
    "        \n",
    "        y = np.array([inverse_K_2, norm.ppf(self.risk_neutral_probabilities(t))])\n",
    "\n",
    "        # Calcul de la perte attendue en fonction de la distribution bivariée\n",
    "        return (bivariate_distribution.cdf(x) - \n",
    "                 bivariate_distribution.cdf(y)) / ((self.detachment - self.attachment)/(1 - recovery))\n",
    "\n",
    "    # Calcul du spread synthétique du CDO en utilisant l'approximation gaussienne\n",
    "    def synthetic_cdo_spread_computation_gaussian_normal(self, proba,recovery):\n",
    "        \n",
    "        spread_list = []\n",
    "    \n",
    "        stopping_times_sim = self.stopping_times_simulation()\n",
    "        \n",
    "        for i in range(len(stopping_times_sim)):\n",
    "            premium_list = []\n",
    "            protection_list = []\n",
    "\n",
    "            stopping_times_sample = stopping_times_sim[i]\n",
    "\n",
    "            for j in range(len(stopping_times_sample) - 1):\n",
    "                current_time = stopping_times_sample[j]\n",
    "                next_time = stopping_times_sample[j + 1]\n",
    "                current_expected_loss = self.expected_loss(current_time,recovery)\n",
    "                next_expected_loss = self.expected_loss(next_time,recovery)\n",
    "                time_diff = next_time - current_time\n",
    "                discount_factor = np.exp(-self.r * current_time)\n",
    "\n",
    "                # Calcul de la différence des flux de primes entre les deux temps d'arrêt\n",
    "                premium_flow_difference = (next_expected_loss - current_expected_loss) * discount_factor\n",
    "                premium_list.append(premium_flow_difference)\n",
    "\n",
    "                # Calcul de la différence des flux de protection entre les deux temps d'arrêt\n",
    "                protection_flow_difference = time_diff * (1 - current_expected_loss) * discount_factor\n",
    "                protection_list.append(protection_flow_difference)\n",
    "            \n",
    "            epsilon = 1e-10\n",
    "            premium_leg = np.sum(premium_list)\n",
    "            protection_leg = np.sum(protection_list)\n",
    "            \n",
    "            \n",
    "            if protection_leg != 0 and not math.isnan(premium_leg) and not math.isnan(protection_leg):\n",
    "                spread_list.append(premium_leg / protection_leg + epsilon)\n",
    "            \n",
    "            # Calcul de la moyenne, de la variance et de l'intervalle de confiance pour le spread synthétique\n",
    "            mean = np.mean(spread_list)\n",
    "            var = np.var(spread_list, ddof=1)\n",
    "            alpha = 1 - proba \n",
    "            quantile = norm.ppf(1 - alpha / 2)  # fonction quantile \n",
    "            ci_size = quantile * np.sqrt(var / np.array(spread_list).size)\n",
    "        \n",
    "        return pd.DataFrame({'Monte-Carlo Spread': mean,\n",
    "                             'Variance': var,\n",
    "                             'lower_confidence': mean - ci_size,\n",
    "                             'upper_confidence': mean + ci_size}, index=['Gaussienne']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cd92fbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = GaussianCDOPricing(attachment=0.3,detachment=0.7,rho=0.3,intensity=0.2,\n",
    "                              T = 5, d= 40,r=0.02,notionel=1,n_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9641e296",
   "metadata": {},
   "source": [
    "Dans notre code, nous avons pris : \n",
    "\n",
    "- attachement  = 0.3  (compris entre 0 et 1 dans l'article) \n",
    "\n",
    "- detachement  0.7  (compris entre 0 et 1 dans l'article)\n",
    "\n",
    "- rho = 0.3 (corrélation entre les actifs du portefeuille de référence) \n",
    "\n",
    "- intensity = 0.2  (issu du triangle de crédit) \n",
    "\n",
    "- T = 5  (Maturité du CDO synthétique) \n",
    "\n",
    "- d = 40 (nombre d'actifs du portefeuille de référence) \n",
    "\n",
    "- notionel = 1 (Le montant du nominal est l'unité)\n",
    "\n",
    "- n_samples = 100 (nombre de simulations égales a 100)\n",
    "\n",
    "- r = 0.02\n",
    "\n",
    "- taux de recouvrement = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85feaf2",
   "metadata": {},
   "source": [
    "Ci-joint le résultat de la simulation de la copule gaussienne. Nous remarquons qu'avec la réduction de la variance, notre variance de simulation est très faible. Ce qui est une très bonne chose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8001a40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.32 s, sys: 5.79 ms, total: 2.33 s\n",
      "Wall time: 2.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_normal = gaussian.synthetic_cdo_spread_computation_gaussian_normal(0.95,recovery= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d811f3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gaussienne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.125615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.120774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.130456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Gaussienne\n",
       "Monte-Carlo Spread    0.125615\n",
       "Variance              0.000610\n",
       "lower_confidence      0.120774\n",
       "upper_confidence      0.130456"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3adf77",
   "metadata": {},
   "source": [
    "**Copule de Student t a un facteur et Approximation LHP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9887a2f8",
   "metadata": {},
   "source": [
    "La copule de Student a été une des copules les plus introduites comme modèle de pricing de CDO synthétiques. La raison profonde est un incovénient majeur de la copule gaussienne. La copule de Student-t est l'une des copules les plus utilisées. Elle est basée sur la version multivariée de la distribution de Student-t. Comme la copule gaussienne, la copule de Student-t fait également partie de la famille des copules elliptiques. Cependant, la distribution de Student-t peut présenter des queues plus épaisses que la distribution gaussienne et est donc mieux adaptée pour modéliser la dépendance des queues. En effet, si nous calculons les corrélations implicites des prix du marché des tranches d'un même CDO en utilisant l'approche LHP, nous n'obtenons pas la même corrélation sur l'ensemble de la structure, mais plutôt, nous observons une distorsion de corrélation (correlation skew). La principale explication de ce phénomène est le manque de dépendance dans les queues de la copule gaussienne. En effet, la copule gaussienne a des queues fines, ce qui en soit, sur les marchés financiers, ne reproduit pas les quantités indispensables pour le trading ou le hedging. \n",
    "\n",
    "Comme la copule gaussienne, la copule de Student-$t$ fait également partie de la famille des copules elliptiques. Cependant, la distribution de Student-$t$ peut présenter des queues plus épaisses que la distribution gaussienne et est donc mieux adaptée pour modéliser la dépendance des queues.\n",
    "\n",
    "On dit qu'une variable aléatoire $X$ suit une distribution de Student-$t$ si elle est générée selon la formule suivante :\n",
    "$$\n",
    "X=\\frac{Z}{\\sqrt{\\xi_v / v}}\n",
    "$$\n",
    "où $Z \\sim N(0,1)$ et $\\xi_v$ est une variable aléatoire indépendante distribuée selon une loi du khi-deux avec $v$ degrés de liberté. La fonction de densité de probabilité de la distribution de Student-$t$ est donnée par :\n",
    "$$\n",
    "f_v(t)=\\frac{\\Gamma((v+1) / 2)}{\\sqrt{v \\pi} \\Gamma(v / 2)}\\left(1+\\frac{t^2}{v}\\right)^{-(v+1) / 2}\n",
    "$$\n",
    "où $\\Gamma(x)$ est la fonction gamma. La distribution de Student-$t$ a les propriétés suivantes :\n",
    "\n",
    "- Elle est symétrique autour de $t=0$ et a donc une moyenne nulle.\n",
    "- Elle a une variance égale à $\\frac{v}{v-2}$ où le paramètre $v$ est connu sous le nom de degrés de liberté. La variance est donc définie seulement si $v>2$.\n",
    "- À la limite $v \\rightarrow \\infty$, la distribution converge vers une distribution gaussienne. Ceci est illustré dans la Figure qui représente la fonction de densité de probabilité pour différentes valeurs de $v$.\n",
    "\n",
    "\n",
    "Le modèle LHP de Student est basé sur une copule de Student. Supposons que le portefeuille d'actifs de référence se compose de $m$ instruments financiers et que le rendement de l'actif jusqu'au temps $t$ du $i$-ème émetteur du portefeuille, $A_i(t)$, ait la forme :\n",
    "\\begin{equation}\n",
    "A_i(t) = a_i M(t) + \\sqrt{1 - a_i^2} X_i(t), \\qquad \n",
    "\\end{equation}\n",
    "où $M(t)$, $X_i(t)$, $i = 1, \\dots, m$, sont des variables aléatoires indépendantes suivant une distribution de Student d'un certain dégré de liberté. Alors, conditionnellement au facteur de marché commun $M(t)$, les rendements des actifs des différents émetteurs sont indépendants.\n",
    "\n",
    "\n",
    "Malheureusement, contrairement a la copule gaussienne, la somme de deux variables aléatoires de Student ne donne pas nécessairement une loi de Student. Cela pose un gros problème lors du calcul des quantiles puisque l'on ne dispose pas de la fonction de répartition $A_i(t)$. Une pratique commune en pratique consiste a choisir une loi de probabilité qui se rapproche de plus des $A_i(t)$. \n",
    "\n",
    "\n",
    "**Dans notre cas, nous avons décidé de choisir une loi qui se rapproche le plus des $A_i(t)$. Notre choix s'est fait de telle sorte a considérer une loi de probabilité dont les paramètres dépendent fortement des dégrés de liberté de la loi de Student. Nous n'avons pas voulu choisir une loi de probabilité avec des paramètres moyenne et variance a estimer. En se placant toujours dans un cadre simplifié, nous allons considérer une loi de student. Autrement dit, nous allons considérer que les $A_i(t)$ suivent une loi de student.**\n",
    "\n",
    "Dans ce cas la, l'expected loss n'est plus une formule fermée comme dans le cadre gaussien et se doit d'etre approximé numériquement. \n",
    "\n",
    "\n",
    "Supposons qu'une fonction de distribution de pertes de portefeuille continue $F(t, x)$ soit connue. Alors, la perte attendue en pourcentage de la tranche CDO $\\left(K_{1}, K_{2}\\right)$ peut être calculée comme suit :\n",
    "\n",
    "$$\n",
    "E L_{\\left(K_{1}, K_{2}\\right)}(t)=\\frac{1}{K_{2}-K_{1}} \\int_{K_{1}}^{1}\\left(\\min \\left(x, K_{2}\\right)-K_{1}\\right) d F(t, x)\n",
    "$$\n",
    "\n",
    "La perte attendue de la tranche dans l'équation peut également être écrite comme suit :\n",
    "\n",
    "$$\n",
    "E L_{\\left(K_{1}, K_{2}\\right)}(t)=\\frac{1}{K_{2}-K_{1}}\\left(\\int_{K_{1}}^{1}\\left(x-K_{1}\\right) d F(t, x)-\\int_{K_{2}}^{1}\\left(x-K_{2}\\right) d F(t, x)\\right) .\n",
    "$$\n",
    "\n",
    "Une extension naturelle de l'approche LHP consiste à utiliser une hypothèse de distribution qui produit une queue lourde. Le modèle à un facteur double $t$ proposé par Hull et White [2004] suppose des distributions de Student t pour le facteur de marché commun $M(t)$ ainsi que pour les facteurs individuels $X_{i}(t)$. Alors, la distribution de perte $F_{\\infty}(t, x)$ dans l'équation devient :\n",
    "\n",
    "$$\n",
    "F_{\\infty}(t, x)=T\\left(\\frac{\\sqrt{1-a^{2}} T^{-1}(x)-C(t)}{a}\\right)\n",
    "$$\n",
    "\n",
    "où $T$ désigne la fonction de distribution de Student t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "399022eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student_t_copula:\n",
    "    \n",
    "    def __init__(self, attachment, detachment, rho, intensity, T, d, r, notionel, n_samples, nu):\n",
    "        self.attachment = attachment\n",
    "        self.detachment = detachment\n",
    "        self.rho = rho\n",
    "        self.intensity = intensity\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.r = r\n",
    "        self.notionel = notionel\n",
    "        self.n_samples = n_samples\n",
    "        self.nu = nu\n",
    "    \n",
    "\n",
    "    def stopping_times_simulation(self):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        mean = np.repeat(0, self.d)\n",
    "        covariance_matrix = np.eye(self.d)\n",
    "\n",
    "        half_n_samples = self.n_samples // 2\n",
    "        multivariate_samples = np.random.multivariate_normal(mean, covariance_matrix, half_n_samples)\n",
    "        antithetic_samples = - multivariate_samples\n",
    "\n",
    "        multivariate_samples = np.vstack((multivariate_samples, antithetic_samples))\n",
    "\n",
    "        covariance_matrix = np.full((self.d, self.d), self.rho) + (1 - self.rho) * np.eye(self.d)\n",
    "\n",
    "        if np.allclose(covariance_matrix, covariance_matrix.T):\n",
    "            try:\n",
    "                L = np.linalg.cholesky(covariance_matrix)\n",
    "\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"La matrice n'est pas sémi définie positive\")\n",
    "\n",
    "        multivariate_samples_correlated = L @ multivariate_samples.T\n",
    "        multivariate_samples_correlated = multivariate_samples_correlated.T\n",
    "\n",
    "        stopping_times = (-1 / self.intensity) * np.log(norm.cdf(multivariate_samples_correlated))\n",
    "\n",
    "        for i in range(stopping_times.shape[0]):\n",
    "            stopping_times[i] = np.sort(stopping_times[i])\n",
    "\n",
    "        filtered_stopping_times = []\n",
    "\n",
    "        for i in range(stopping_times.shape[0]):\n",
    "            filtered_times = stopping_times[i][stopping_times[i] < self.T]\n",
    "            filtered_stopping_times.append(filtered_times)\n",
    "\n",
    "        filtered_stopping_times = np.array(filtered_stopping_times, dtype=object)\n",
    "\n",
    "        return filtered_stopping_times\n",
    "    \n",
    "    \n",
    "    def risk_neutral_probabilities(self,t):\n",
    "        \n",
    "        return 1 - np.exp(- self.intensity * t)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def student_copula_percentile(self,time,index):\n",
    "        \n",
    "        proba = self.risk_neutral_probabilities(time)\n",
    "        \n",
    "        quantile = t.ppf(proba,self.nu)\n",
    "        \n",
    "        return quantile\n",
    "    \n",
    "    \n",
    "    def expected_loss(self,time,index,recovery=0):\n",
    "        \n",
    "        def f(x):\n",
    "            \n",
    "            return (min(x,(self.detachment/(1 - recovery))) - (self.attachment/(1 - recovery)))\n",
    "        \n",
    "        def g(x):\n",
    "            \n",
    "            c_t = self.student_copula_percentile(time,index)\n",
    "            \n",
    "            return (np.sqrt(1-self.rho**2)/self.rho) * (1/t.pdf(t.ppf(x,self.nu),self.nu)) * t.pdf((np.sqrt(1-self.rho**2) * t.ppf(x,self.nu) - c_t)/self.rho,self.nu)\n",
    "        \n",
    "        def integrand(x):\n",
    "            \n",
    "            return (f(x) * g(x))/((self.detachment - self.attachment)/(1 - recovery))\n",
    "        \n",
    "        \n",
    "        a = (self.attachment/(1 - recovery))\n",
    "        b = 1\n",
    "        \n",
    "        return quad(integrand,a,b)[0]\n",
    "    \n",
    "    \n",
    "    def synthetic_cdo_spread_computation_student(self,proba,recovery=0):\n",
    "        \n",
    "         spread_list = []\n",
    "    \n",
    "         stopping_times_sim = self.stopping_times_simulation()\n",
    "        \n",
    "         for i in range(len(stopping_times_sim)):\n",
    "            premium_list = []\n",
    "            protection_list = []\n",
    "\n",
    "            stopping_times_sample = stopping_times_sim[i]\n",
    "\n",
    "            for j in range(len(stopping_times_sample) - 1):\n",
    "                current_time = stopping_times_sample[j]\n",
    "                next_time = stopping_times_sample[j + 1]\n",
    "                current_expected_loss = self.expected_loss(current_time,i,recovery)\n",
    "                next_expected_loss = self.expected_loss(next_time,i,recovery)\n",
    "                time_diff = next_time - current_time\n",
    "                discount_factor = np.exp(-self.r * current_time)\n",
    "\n",
    "                premium_flow_difference = (next_expected_loss - current_expected_loss) * discount_factor\n",
    "                premium_list.append(premium_flow_difference)\n",
    "\n",
    "                protection_flow_difference = time_diff * (1 - current_expected_loss) * discount_factor\n",
    "                protection_list.append(protection_flow_difference)\n",
    "            \n",
    "            epsilon = 1e-10\n",
    "            premium_leg = np.sum(premium_list)\n",
    "            protection_leg = np.sum(protection_list)\n",
    "           \n",
    "            if protection_leg != 0 and not math.isnan(premium_leg) and not math.isnan(protection_leg):\n",
    "                spread_list.append(premium_leg / protection_leg + epsilon)\n",
    "            \n",
    "            \n",
    "            mean = np.mean(spread_list)\n",
    "            var = np.var(spread_list, ddof=1)\n",
    "            alpha = 1 - proba \n",
    "            quantile = norm.ppf(1 - alpha/2)  # fonction quantile \n",
    "            ci_size = quantile * np.sqrt(var / np.array(spread_list).size)\n",
    "        \n",
    "         return pd.DataFrame({'Monte-Carlo Spread':mean,\n",
    "                             'Variance':var,\n",
    "                             'lower_confidence':mean - ci_size,\n",
    "                             'upper_confidence':mean + ci_size},index=['Student']).T\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f971c8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nig = Student_t_copula(attachment=0.3,detachment=0.7,rho=0.3, intensity=0.2, T=5,\n",
    "                                     d = 40, r=0.02, notionel=1, n_samples=5, nu=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f9658168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 s, sys: 1.74 s, total: 32.7 s\n",
      "Wall time: 31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_nig_student = nig.synthetic_cdo_spread_computation_student(0.95,recovery = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ce21a34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.126597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.118941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.134254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Student\n",
       "Monte-Carlo Spread  0.126597\n",
       "Variance            0.000061\n",
       "lower_confidence    0.118941\n",
       "upper_confidence    0.134254"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nig_student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0d7b7",
   "metadata": {},
   "source": [
    "**Méthode la copule de la normale inverse gaussienne a un facteur et Approximation LHP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5feea80",
   "metadata": {},
   "source": [
    "Comme nous l'avons dit précédemment, la copule gaussienne présente des incovénients majeurs. En raison de sa simplicité, ce modèle est devenu la norme du marché. Cependant, il existe un problème fondamental :\n",
    "Si nous calculons les corrélations implicites des prix du marché des tranches d'un même CDO en utilisant l'approche LHP, nous n'obtenons pas la même corrélation sur l'ensemble de la structure, mais plutôt, nous observons une distorsion de corrélation (correlation skew). La principale explication de ce phénomène est le manque de dépendance dans les queues de la copule gaussienne. En effet, la distribution gaussienne a des queues fines, ce qui ne produit des résultats assez corrects. Comme vu précédemment, la copule de student corrige cette effet indésirable de la loi normale centrée et réduite en utilisant une distribution a queues épaisses. Mais le véritable problème de la copule de Student, c'est que les $A_i(t)$ ne sont stables par convolution. Autrement dit, les $A_i(t)$ ne suivent pas une loi de student. Il est indispensable de choisir dans ce cas, une distribution de probabilité qui se rapproche le plus de la somme de deux variables de student corrélées pour calculer les quantiles assosciés. \n",
    "\n",
    "La normale inverse gaussienne est une loi de probabilité particulière. Les distributions NIG (Normal Inverse Gaussian) sont très intéressantes pour les applications en finance - elles constituent une famille de distributions flexibles à quatre paramètres qui peuvent produire des queues épaisses et de l'asymétrie. Cette classe de distributions est stable en convolution sous certaines conditions, et les fonctions de répartition cumulatives, de densité et de distribution inverse peuvent encore être calculées suffisamment rapidement (Kalemanova et Werner [2006])\n",
    "\n",
    "- **stable par convolution** en choisissant des paramètres de manière stratégique. Autrement dit, les $A_i(t)$ suivent une NIG, ce qui permet de déduire facilement les quantiles associés dans la formule de l'expected loss \n",
    "\n",
    "- **Dispose de queues de distribution épaisses** ce qui corrige l'effet de corrélation skew\n",
    "\n",
    "- On peut disposer de formule semi-fermées en faisant les bonnes approximations sur les fonctions de répartitions, ce qui peut nous permettre de calculer l'expected loss. \n",
    "\n",
    "\n",
    "La densité d'une variable aléatoire $X \\sim \\mathcal{N} \\mathcal{I} \\mathcal{G}(\\alpha, \\beta, \\mu, \\delta)$ est donnée par :\n",
    "$$\n",
    "f_{\\mathcal{N I G}}(x ; \\alpha, \\beta, \\mu, \\delta)=\\frac{\\delta \\alpha \\cdot \\exp (\\delta \\gamma+\\beta(x-\\mu))}{\\pi \\cdot \\sqrt{\\delta^2+(x-\\mu)^2}} K_1\\left(\\alpha \\sqrt{\\delta^2+(x-\\mu)^2}\\right)\n",
    "$$\n",
    "où $K_1(w):=\\frac{1}{2} \\int_0^{\\infty} \\exp \\left(-\\frac{1}{2} w\\left(t+t^{-1}\\right)\\right) d t$ est la fonction de Bessel modifiée du troisième type.\n",
    "\n",
    "\n",
    "avec des paramètres satisfaisant les conditions suivantes : $0 \\leq |\\beta| < \\alpha$ et $\\delta > 0$. Nous écrivons alors $X \\sim \\mathcal{N} \\mathcal{I} \\mathcal{G}(\\alpha, \\beta, \\mu, \\delta)$ et notons respectivement les fonctions de densité et de probabilité par $f_{\\mathcal{N} \\mathcal{I} \\mathcal{G}}(x; \\alpha, \\beta, \\mu, \\delta)$ et $F_{\\mathcal{N} \\mathcal{I} \\mathcal{G}}(x; \\alpha, \\beta, \\mu, \\delta)$.\n",
    "\n",
    "\n",
    "**Modèle de la copule normale inverse gaussienne a un facteur**\n",
    "\n",
    "Considérons un portefeuille homogène de $m$ instruments de crédit. Le rendement de l'actif standardisé jusqu'au temps $t$ du $i$-ème émetteur du portefeuille, $A_i(t)$, est supposé être de la forme :\n",
    "$$\n",
    "A_i(t) = aM(t) + \\sqrt{1-a^2}X_i(t),\n",
    "$$\n",
    "avec des variables aléatoires indépendantes\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& M(t) \\sim \\mathcal{N I G}\\left(\\alpha, \\beta,-\\frac{\\beta \\gamma^2}{\\alpha^2}, \\frac{\\gamma^3}{\\alpha^2}\\right) \\\\\n",
    "& X_i(t) \\sim \\mathcal{N I G}\\left(\\frac{\\sqrt{1-a^2}}{a} \\alpha, \\frac{\\sqrt{1-a^2}}{a} \\beta,-\\frac{\\sqrt{1-a^2}}{a} \\frac{\\beta \\gamma^2}{\\alpha^2}, \\frac{\\sqrt{1-a^2}}{a} \\frac{\\gamma^3}{\\alpha^2}\\right) \\\\\n",
    "&\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\gamma=\\sqrt{\\alpha^2-\\beta^2}$.\n",
    "\n",
    "Dans ces conditions, les $A_i(t) \\sim \\mathcal{N I G}$ de paramètres : \n",
    "\n",
    "$$A_i(t) \\sim \\mathcal{N I G}\\left(\\frac{\\alpha}{a}, \\frac{\\beta}{a},\\frac{-1}{a}   \\frac{\\beta \\gamma^2}{\\alpha^2}, \\frac{1}{a} \\frac{\\gamma^3}{\\alpha^2}\\right)$$\n",
    "\n",
    "\n",
    "Notez qu'il a également une moyenne nulle et une variance unitaire.\n",
    "\n",
    "\n",
    "Voici comment nous avons procéder pour pricer le CDO synthétique avec un modèle NIG avec monte carlo : \n",
    "\n",
    "\n",
    "# Classe NormalInverseGaussianCDOPricing\n",
    "\n",
    "Cette classe simule le pricing d'un CDO synthétique en utilisant la copule Normale Inverse Gaussienne.\n",
    "\n",
    "###  __init__\n",
    "Initialise les attributs de la classe :\n",
    "\n",
    "- attachment : point d'attachement\n",
    "- detachment : point de détachement\n",
    "- rho : corrélation\n",
    "- intensity : intensité de défaut\n",
    "- T : maturité du CDO\n",
    "- d : nombre d'actifs du CDO\n",
    "- r : taux d'intérêt constant\n",
    "- notionel : nominal\n",
    "- n_samples : nombre de simulations\n",
    "- alpha : paramètre alpha de la distribution Normale Inverse Gaussienne\n",
    "- beta : paramètre beta de la distribution Normale Inverse Gaussienne\n",
    "\n",
    "\n",
    "### stopping_times_simulation\n",
    "Simule les temps d'arrêt en utilisant la copule Normale Inverse Gaussienne :\n",
    "\n",
    "- Initialise la matrice de corrélation et génère des échantillons multivariés gaussiens.\n",
    "- Utilise la méthode antithétique pour améliorer l'efficacité de la simulation.\n",
    "- Corrèle les échantillons multivariés en utilisant la décomposition de Cholesky.\n",
    "- Calcule les temps d'arrêt en utilisant la fonction de répartition inverse de la distribution gaussienne standard.\n",
    "- Filtre les temps d'arrêt pour ne garder que ceux qui sont inférieurs à la maturité du CDO.\n",
    "- Retourne un tableau numpy des temps d'arrêt filtrés.\n",
    "- risk_neutral_probabilities\n",
    "- Calcule les probabilités neutres au risque en utilisant l'intensité de défaut.\n",
    "\n",
    "### nig_cumulative_distribution\n",
    "Cette fonction calcule la fonction de répartition cumulative (FRC) de la distribution normale inverse gaussienne (NIG) pour une valeur donnée x. La FRC de la distribution NIG est définie comme la probabilité que la variable aléatoire NIG soit inférieure ou égale à x.\n",
    "\n",
    "### nig_density_distribution\n",
    "Cette fonction calcule la fonction de densité de probabilité (FDP) de la distribution normale inverse gaussienne pour une valeur donnée x.\n",
    "\n",
    "\n",
    "### inverse_nig_distribution\n",
    "Cette fonction calcule la fonction de répartition inverse (FRI) de la distribution normale inverse gaussienne pour une probabilité donnée p. La FRI est l'inverse de la FRC et est utilisée pour trouver la valeur x correspondant à une probabilité p.\n",
    "\n",
    "### nig_expected_loss\n",
    "Calcule la perte attendue à un instant donné en utilisant la fonction de répartition cumulative et la fonction de répartition inverse de la distribution Normale Inverse Gaussienne. Cette perte est donnée par : \n",
    "\n",
    "\n",
    "$$\n",
    "E L_{\\left(K_1, K_2\\right)}(t)=\\frac{1}{K_2-K_1} \\int_{K_1}^{K_2}\\left(x-K_1\\right) d F_{\\infty}(t, x)+\\left(1-F_{\\infty}\\left(t, K_2\\right)\\right) .\n",
    "$$\n",
    "\n",
    "\n",
    "avec $$F_{\\infty}(t, x)=1-F_{\\mathcal{N I G}(1)}\\left(\\frac{C(t)-\\sqrt{1-a^2} F_{\\mathcal{N I G}\\left(\\frac{\\sqrt{1-a^2}}{a}\\right)}(x)}{a}\\right)$$\n",
    "\n",
    "\n",
    "\n",
    "$C(t)=F_{\\mathcal{N I G}\\left(\\frac{1}{a}\\right)}^{-1}(q(t))$, ou $q(t)$ est la probabilité risque neutre de défaut\n",
    "\n",
    "avec $q(t)$\n",
    "\n",
    "\\begin{equation}\n",
    "q(t) = 1 - \\exp(-\\lambda * t)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "&  \\\\\n",
    "& \\int_{K_1}^{K_2}\\left(x-K_1\\right) d F_{\\infty}(t, x) = \\quad \\int_{F_{\\mathcal{N I G}\\left(\\frac{\\sqrt{1-a^2}}{a}\\right)(K1)}^{-1}}^{F_{\\mathcal{N I G}\\left(\\frac{\\sqrt{1-a^2}}{a}\\right)(K2)}^{-1}}\\left(F_{\\mathcal{N I G}\\left(\\frac{\\sqrt{1-a^2}}{a}\\right)}(y)-K_1\\right) f_{\\mathcal{N I G}(1)}\\left(\\frac{C(t)-\\sqrt{1-a^2} y}{a}\\right) \\frac{\\sqrt{1-a^2}}{a} d x .\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### synthetic_cdo_spread_normal_inverse_gaussian\n",
    "Calcule le spread du CDO synthétique en utilisant la méthode de Monte-Carlo :\n",
    "\n",
    "- Initialise une liste pour stocker les spreads.\n",
    "- Pour chaque échantillon de temps d'arrêt simulé :\n",
    "  - Initialise des listes pour stocker les flux de primes et de protection.\n",
    "- Pour chaque paire de temps d'arrêt adjacents :\n",
    "  - Calcule les pertes attendues aux temps d'arrêt courant et suivant.\n",
    "  - Calcule la différence de temps entre les temps d'arrêt courant et suivant.\n",
    "  - Calcule le facteur d'actualisation en utilisant le taux d'intérêt constant $r$ et le temps d'arrêt courant.\n",
    "  - Calcule la différence des flux de primes et de protection en utilisant les pertes attendues, la différence de temps et le facteur d'actualisation.\n",
    "  - Ajoute les différences de flux de primes et de protection aux listes correspondantes.\n",
    "  - Calcule la jambe de prime et la jambe de protection en sommant les flux de primes et de protection, respectivement.\n",
    "  - Calcule le spread en divisant la jambe de prime par la jambe de protection, et ajoute le résultat à la liste des spreads.\n",
    "- Calcule la moyenne, la variance et l'intervalle de confiance des spreads.\n",
    "- Retourne un DataFrame pandas contenant les résultats.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7808786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalInverseGaussianCDOPricing:\n",
    "    \n",
    "    def __init__(self, attachment, detachment, rho, intensity, T, d, r, \n",
    "                 notionel, n_samples, alpha, beta):\n",
    "        \n",
    "        self.attachment = attachment\n",
    "        self.detachment = detachment\n",
    "        self.intensity = intensity\n",
    "        self.rho = rho\n",
    "        self.T = T\n",
    "        self.d = d\n",
    "        self.r = r\n",
    "        self.notionel = notionel\n",
    "        self.n_samples = n_samples\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        \n",
    "    def stopping_times_simulation(self):\n",
    "        np.random.seed(42)\n",
    "\n",
    "        mean = np.repeat(0, self.d)\n",
    "        covariance_matrix = np.eye(self.d)\n",
    "\n",
    "        half_n_samples = self.n_samples // 2\n",
    "        multivariate_samples = np.random.multivariate_normal(mean, covariance_matrix, half_n_samples)\n",
    "        antithetic_samples = -multivariate_samples\n",
    "\n",
    "        multivariate_samples = np.vstack((multivariate_samples, antithetic_samples))\n",
    "\n",
    "        covariance_matrix = np.full((self.d, self.d), self.rho) + (1 - self.rho) * np.eye(self.d)\n",
    "\n",
    "        if np.allclose(covariance_matrix, covariance_matrix.T):\n",
    "            try:\n",
    "                L = np.linalg.cholesky(covariance_matrix)\n",
    "\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"La matrice n'est pas sémi définie positive\")\n",
    "\n",
    "        multivariate_samples_correlated = L @ multivariate_samples.T\n",
    "        multivariate_samples_correlated = multivariate_samples_correlated.T\n",
    "\n",
    "        stopping_times = (-1 / self.intensity) * np.log(norm.cdf(multivariate_samples_correlated))\n",
    "\n",
    "        for i in range(stopping_times.shape[0]):\n",
    "            stopping_times[i] = np.sort(stopping_times[i])\n",
    "\n",
    "        filtered_stopping_times = []\n",
    "\n",
    "        for i in range(stopping_times.shape[0]):\n",
    "            filtered_times = stopping_times[i][stopping_times[i] < self.T]\n",
    "            filtered_stopping_times.append(filtered_times)\n",
    "\n",
    "        filtered_stopping_times = np.array(filtered_stopping_times, dtype=object)\n",
    "\n",
    "        return filtered_stopping_times\n",
    "\n",
    "    \n",
    "    def risk_neutral_probabilities(self, t):\n",
    "        \n",
    "        return  1 - np.exp(-self.intensity * t)\n",
    "    \n",
    "    def nig_cumulative_distribution(self, x, s):\n",
    "        \n",
    "        gamma = np.sqrt(self.alpha ** 2 - self.beta ** 2)\n",
    "        alpha, beta = s * self.alpha, s * self.beta\n",
    "        loc = -s * ((-self.beta * gamma ** 2) / (self.alpha ** 2))\n",
    "        scale = s * (gamma ** 3 / (self.alpha ** 2))\n",
    "        \n",
    "        return norminvgauss.cdf(x, alpha * scale, beta * scale, loc, scale)\n",
    "    \n",
    "    def nig_density_distribution(self, x, s):\n",
    "        \n",
    "        gamma = np.sqrt(self.alpha ** 2 - self.beta ** 2)\n",
    "        alpha, beta = s * self.alpha, s * self.beta\n",
    "        loc = -s * ((-self.beta * gamma ** 2) / (self.alpha ** 2))\n",
    "        scale = s * (gamma ** 3 / (self.alpha ** 2))\n",
    "        \n",
    "        return norminvgauss.pdf(x, alpha * scale, beta * scale, loc, scale)\n",
    "    \n",
    "    def inverse_nig_distribution(self, x, s):\n",
    "        \n",
    "        gamma = np.sqrt(self.alpha ** 2 - self.beta ** 2)\n",
    "        alpha, beta = s * self.alpha, s * self.beta\n",
    "        loc = -s * ((-self.beta * gamma ** 2) / (self.alpha ** 2))\n",
    "        scale = s * (gamma ** 3 / (self.alpha ** 2))\n",
    "        \n",
    "        return norminvgauss.ppf(x, alpha * scale, beta * scale, loc, scale)\n",
    "    \n",
    "    def nig_expected_loss(self,time, recovery=0):\n",
    "        \n",
    "        f = lambda x: self.nig_cumulative_distribution(x, (np.sqrt(1 - self.rho**2) / self.rho)) - (self.attachment/(1 - recovery))\n",
    "        g = lambda x: self.nig_density_distribution((self.inverse_nig_distribution(self.risk_neutral_probabilities(time), 1 / self.rho) - np.sqrt(1 - self.rho**2) * x) / self.rho, 1) * (np.sqrt(1 - self.rho**2) / self.rho)\n",
    "        f_infinity = lambda x, time: 1 - self.nig_cumulative_distribution(((self.inverse_nig_distribution(self.risk_neutral_probabilities(time), 1 / self.rho) - np.sqrt(1 - self.rho**2) * self.inverse_nig_distribution(x, (np.sqrt(1 - self.rho**2) / self.rho))) / self.rho), 1)\n",
    "        \n",
    "        b = self.inverse_nig_distribution((self.detachment/(1 - recovery)), (np.sqrt(1 - self.rho**2) / self.rho))\n",
    "        z = self.inverse_nig_distribution((self.attachment/(1 - recovery)), (np.sqrt(1 - self.rho**2) / self.rho))\n",
    "        \n",
    "        integrand = lambda x: (f(x) * g(x)) / ((self.detachment - self.attachment)/(1 - recovery))\n",
    "        infinity = f_infinity((self.detachment/(1 - recovery)), time)\n",
    "        \n",
    "        return quad(integrand, z, b)[0] + (1 - infinity)\n",
    "    \n",
    "    def synthetic_cdo_spread_normal_inverse_gaussian(self,proba,recovery=0):\n",
    "    \n",
    "        spread_list = []\n",
    "        stopping_times_sim = self.stopping_times_simulation()\n",
    "        \n",
    "        for i in range(len(stopping_times_sim)):\n",
    "            premium_list = []\n",
    "            protection_list = []\n",
    "\n",
    "            stopping_times_sample = stopping_times_sim[i]\n",
    "\n",
    "            for j in range(len(stopping_times_sample) - 1):\n",
    "                current_time = stopping_times_sample[j]\n",
    "                next_time = stopping_times_sample[j + 1]\n",
    "                current_expected_loss = self.nig_expected_loss(current_time,recovery)\n",
    "                next_expected_loss = self.nig_expected_loss(next_time,recovery)\n",
    "                time_diff = next_time - current_time\n",
    "                discount_factor = np.exp(-self.r * current_time)\n",
    "\n",
    "                premium_flow_difference = (next_expected_loss - current_expected_loss) * discount_factor\n",
    "                premium_list.append(premium_flow_difference)\n",
    "\n",
    "                protection_flow_difference = time_diff * (1 - current_expected_loss) * discount_factor\n",
    "                protection_list.append(protection_flow_difference)\n",
    "\n",
    "            epsilon = 1e-10    \n",
    "            premium_leg = np.sum(premium_list)\n",
    "            protection_leg = np.sum(protection_list)\n",
    "            \n",
    "            if protection_leg != 0 and not math.isnan(premium_leg) and not math.isnan(protection_leg):\n",
    "                spread_list.append(premium_leg / protection_leg + epsilon)\n",
    "        \n",
    "        mean = np.mean(spread_list)\n",
    "        var = np.var(spread_list, ddof=1)\n",
    "        alpha = 1 - proba\n",
    "        quantile = norm.ppf(1 - alpha / 2)\n",
    "        ci_size = quantile * np.sqrt(var / len(spread_list))\n",
    "    \n",
    "        return pd.DataFrame({'Monte-Carlo Spread': mean,\n",
    "                             'Variance': var,\n",
    "                             'lower_confidence': mean - ci_size,\n",
    "                             'upper_confidence': mean + ci_size}, index=['Norm.Inv.Gauss.']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4846b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nig = NormalInverseGaussianCDOPricing(attachment=0.3,detachment=0.7,rho=0.3, intensity=0.2,T=5,\n",
    "                                     d = 40, r=0.02,notionel=1, n_samples=5,alpha=1,beta=0.625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "baeee8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 42s, sys: 1.41 s, total: 4min 43s\n",
      "Wall time: 6min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_nig = nig.synthetic_cdo_spread_normal_inverse_gaussian(0.95,recovery = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0c4187e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Norm.Inv.Gauss.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.129692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.121880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.137504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Norm.Inv.Gauss.\n",
       "Monte-Carlo Spread         0.129692\n",
       "Variance                   0.000064\n",
       "lower_confidence           0.121880\n",
       "upper_confidence           0.137504"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nig "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144c897",
   "metadata": {},
   "source": [
    "- attachement  = 0.3  (compris entre 0 et 1 dans l'article) \n",
    "\n",
    "- detachement  0.7  (compris entre 0 et 1 dans l'article)\n",
    "\n",
    "- rho = 0.3 (corrélation entre les actifs du portefeuille de référence) \n",
    "\n",
    "- intensity = 0.2  (issu du triangle de crédit) \n",
    "\n",
    "- T = 5  (Maturité du CDO synthétique) \n",
    "\n",
    "- d = 40 (nombre d'actifs du portefeuille de référence) \n",
    "\n",
    "- notionel = 1 (Le montant du nominal est l'unité)\n",
    "\n",
    "- n_samples = 100 (nombre de simulations égales a 100)\n",
    "\n",
    "- alpha = 1 \n",
    "\n",
    "- beta = 0.625\n",
    "\n",
    "- r = 0.02\n",
    "\n",
    "- taux de recouvrement = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dda8c2",
   "metadata": {},
   "source": [
    "Nous obtenons des résultats sensiblement égaux a la loi normale. La simualtion est assez lente. Dans notre cas, nous avons fait 5 simulations mais nous remarquons déjà que les résultats semblent converger **rapidement** vers les résultats de la loi copule gaussienne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e50244c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gaussienne</th>\n",
       "      <th>Norm.Inv.Gauss.</th>\n",
       "      <th>Student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Monte-Carlo Spread</th>\n",
       "      <td>0.125615</td>\n",
       "      <td>0.129692</td>\n",
       "      <td>0.126597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Variance</th>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower_confidence</th>\n",
       "      <td>0.120774</td>\n",
       "      <td>0.121880</td>\n",
       "      <td>0.118941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper_confidence</th>\n",
       "      <td>0.130456</td>\n",
       "      <td>0.137504</td>\n",
       "      <td>0.134254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Gaussienne  Norm.Inv.Gauss.   Student\n",
       "Monte-Carlo Spread    0.125615         0.129692  0.126597\n",
       "Variance              0.000610         0.000064  0.000061\n",
       "lower_confidence      0.120774         0.121880  0.118941\n",
       "upper_confidence      0.130456         0.137504  0.134254"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([df_normal,df_nig,df_nig_student],axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
